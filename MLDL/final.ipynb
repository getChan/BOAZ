{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOAZ ML/DL Project1 - 대장암 데이터(Colorectal cancer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date : 2019.03.28.Thur  \n",
    "Author : 성민석,김아영,조문기,유승룡,남궁찬\n",
    "\n",
    "해당 자료는 [Minsuk Heo](https://github.com/minsuk-heo/kaggle-titanic/blob/master/titanic-solution.ipynb)님의 `Kaggle Competition` 중 하나인 `Titanic 생존자에 관한 EDA`를 참고하였음을 미리 밝힙니다.  \n",
    "또한 [Deep Play](https://3months.tistory.com/325)님의 `Python으로 하는 EDA` 블로그 글을 참고하였음을 밝히며 깊은 감사를 표합니다. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Defining the problem statement\n",
    "\n",
    "- 대장암에 관하여\n",
    "\n",
    "## 요약\n",
    "\n",
    "**대장**은 **충수, 맹장, 결장, 직장, 그리고 항문관**으로 나뉘며, 결장은 다시 상행결장, 횡행결장, 하행결장, 에스상(S狀)결장으로 나뉘는데, 이 가운데 맹장, 결장과 직장에 생기는 악성 종양이 대장암입니다. 대장암의 대부분은 선암(腺癌. 샘암), 즉 점막의 샘세포에 생기는 암이며, 그 밖에 림프종, 악성 유암종(類癌腫), 평활근육종(平滑筋肉腫) 같은 것이 원발성으로 생길 수 있습니다.\n",
    "\n",
    "## 정의\n",
    "**대장암**이란 **결장과 직장에 생기는 악성 종양**을 말합니다. 발생 위치에 따라 결장에 생기면 **결장암**, 직장에 생기면 **직장암**이라고 하며, 이를 통칭하여 대장암 혹은 결장직장암이라고 합니다. 대장의 **대부분이 결장이기 때문에** 맥락에 따라 대장이라는 말로 **결장만을 뜻할 때도 간혹 있습니다**.\n",
    "\n",
    "## 종류\n",
    "대장암의 대부분은 대장 점막 샘세포에서 발생하는 선암(腺癌, 샘암)으로, 양성 종양인 선종성 용종(茸腫, polyp)에서 유래한다고 알려졌습니다. **용종**이란 위장관 점막의 조직이 부분적으로 과도하게 증식하여 혹처럼 튀어나온 것을 말하며, 선종(腺腫, 샘종, adenoma)이란 샘세포가 증식하여 생기는 종양입니다. 전체 대장암의 약 5~15%는 유전적 요인으로 인해 발생합니다.\n",
    "\n",
    "선암 이외에도 림프종, 신경내분비종양(유암종, 類癌腫, carcinoid), 평활근육종(平滑筋肉腫) 등이 원발성으로, 즉 다른 병의 결과가 아니라 그 자체로 생길 수 있습니다. 대장의 림프종은 전체 소화관의 악성 종양 중 1% 미만이며, 소화관 림프종 중에서는 10~20%를 차지합니다. 대장이 시작되는 부분인 회맹부에 잘 생기고 증상은 선암과 동일하며, 때때로 오른쪽 하복부에서 종괴(덩이)로 발견됩니다. 신경내분비종양(유암종)이란 위장관과 췌장, 난소, 폐 등의 신경내분비세포에서 발생하여 서서히 자라는 종양인데, 충수(충양돌기)와 직장에 주로 생기며 대부분 증상이 없습니다. 전이되거나 악성으로 유암종 증후군을 일으키는 경우는 극히 드뭅니다. 평활근육종이란 내장이나 혈관 따위의 벽을 이루는 평활근 즉 민무늬근에 생기는 육종(비상피성 조직에서 유래하는 악성 종양)입니다.\n",
    "\n",
    "한편, 대장에도 카포시 육종(Kaposi's sarcoma)이라는 매우 드문 악성 종양이 생기는 수가 있습니다. 이 육종은 헤르페스바이러스(Kaposi’s sarcoma herpes virus, KSHV)에 의해 발생하는 것으로, 피부에 가장 많이 나타나지만 뇌를 제외한 모든 장기에 생길 수 있으며 후천성면역결핍증(AIDS) 환자에게서 자주 발견됩니다. 대장이나 직장에서는 붉은 반점이나 결절, 또는 용종의 형태를 보입니다.\n",
    "\n",
    "## 발생부위\n",
    "· 대장의 위치 및 구조\n",
    "대장 즉 큰 창자는 소장(작은창자)의 끝에서부터 항문까지 이어진 소화기관으로, **길이가 약 150cm 정도**입니다. 대장은 **맹장**(盲腸, 막창자), **결장**(結腸, 잘록창자, 대장의 대부분), **직장**(直腸, 곧창자), 그리고 **항문관**으로 나뉘며, 결장은 다시 상행결장(오름잘록창자), 횡행결장(가로잘록창자), 하행결장(내림잘록창자), 에스상(S狀)결장(구불잘록창자)으로 나뉩니다. 소장의 마지막 부분인 회장(回腸, 돌창자)의 말단과 대장의 초입인 맹장 사이에는 회맹판(回盲辦)이라는 것이 있어서 대장의 내용물이 소장으로 역류하는 것을 막습니다. 맹장 중앙부로부터 회맹 접합부 아래는 충수(蟲垂)라는 것이 7~8cm 가량 나와 있습니다. 충양돌기, 막창자꼬리라고도 하는 이 부위는 우리가 흔히 맹장염이라고 하는 막창자꼬리염이 발생하는 부위입니다. 에스상결장에 이어지는 직장은 항문관으로 넘어가는 부위인 항문직장륜에서 끝나며, 길이는 13~15cm입니다.\n",
    "\n",
    "대장의 직경은 맹장 부분이 7.8~8.5cm로 가장 크고, 원위부(遠位部, 아랫부분)로 갈수록 점차 작아져서 에스상결장에서는 약 2.5cm가 되었다가 직장에서 4.5cm쯤으로 다시 커지고, 항문관에서는 도로 작아집니다.\n",
    "\n",
    "대장벽은 점막(粘膜), 점막하조직, 근육층(윤상근[輪狀筋]과 종근 혹은 종주근[縱走筋]), 장막(腸膜)의 네 층으로 되어 있습니다.\n",
    "\n",
    "출처 : [국가암정보센터 암정보](https://terms.naver.com/entry.nhn?docId=5646470&cid=60406&categoryId=60406)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis ( EDA )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting the data\n",
    "자료 제공자 : **김아영**  \n",
    "본 데이터는 ○○○○에서 제공한 데이터이므로, 외부 유출이 엄격히 금지되어있습니다. 이 점 양해주시면 좋겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # numpy: 1.16.2 \n",
    "import pandas as pd # pandas: 0.24.2\n",
    "import matplotlib.pyplot as plt # matplotlib: 3.0.3\n",
    "from matplotlib import rc\n",
    "rc('font', family='AppleGothic')\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set() # setting seaborn default for plots\n",
    "import missingno as msno # 결측치를 처리하기 위한"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas 세부사항\n",
    "# https://songhuiming.github.io/pages/2017/04/02/jupyter-and-pandas-display/\n",
    "pd.set_option('display.max_columns', 500) # feature가 많은걸 대비해서 확장해둠\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 버전 확인\n",
    "# pd.show_versions(as_json=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check `Shape` of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'./colorectal_cancer.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-cd55af3f14a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./colorectal_cancer.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'./colorectal_cancer.csv' does not exist"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('./colorectal_cancer.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주어진 csv파일의 크기는 대략 **7만개**입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#주어진 데이터의 첫 10개의 행\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 데이터의 마지막 10개의 행\n",
    "data.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check `Type` of Features\n",
    "주어진 데이터의 형태를 체크해봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 해당 데이터를 **히스토그램**으로 표현해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-8ac293282085>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 각 변수에 대한 히스토그램은 알파벳 순으로 정렬됨\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mxlabelsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mylabelsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# 각 변수에 대한 히스토그램은 알파벳 순으로 정렬됨\n",
    "data.hist(figsize=(16,20),bins=60,xlabelsize=8,ylabelsize=8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-3f1b1355d77a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 더 자세한 label을 확인하기 위해서\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# 더 자세한 label을 확인하기 위해서\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단하게 데이터의 값들에 대해서\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주어진 데이터가 상당히 많기 때문에 하나의 사전/표를 준비해봤습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dictionary\n",
    "\n",
    "데이터 내 **60개 변수**에 대한 간단한 설명  \n",
    "\n",
    "|Features|Meaning|Description|\n",
    "|---|---|---|\n",
    "|no|환자번호||\n",
    "|studyid|연구번호(랜덤)||\n",
    "|sex|성별|`1`-남자 `2`-여자|\n",
    "|age|나이||\n",
    "|height|신장||\n",
    "|weight|체중||\n",
    "|bmi|BMI 지수||\n",
    "|obesity|비만도||\n",
    "|wt_circ|허리둘레||\n",
    "|abd_obe|복부비만||\n",
    "|wbc|백혈구||\n",
    "|rbc|적혈구||\n",
    "|hb|헤모글로빈||\n",
    "|hct|헤마토크리트(적혈구 용적률)||\n",
    "|rdw|적혈구 크기 분포||\n",
    "|plt|혈소판||\n",
    "|ca|칼슘||\n",
    "|glucose|혈당||\n",
    "|glu_100|혈당 100기준|`1`-100미만(정상) `2`-100 초과|\n",
    "|prot|프로틴||\n",
    "|alb|알부민(단백질의 일종)||\n",
    "|glob|글로불린||\n",
    "|t_chol|총 콜레스테롤||\n",
    "|ldh|젖산 탈수소 효소||\n",
    "|tg|트리 글리세라이드||\n",
    "|tg_150|트리 글리세라이드 150 기준|`1`-150 미만(정상) `2`-150 초과|\n",
    "|hdl|고밀도 콜레스테롤||\n",
    "|hdl_abnl|고밀도 콜레스테롤 정상여부|`1`-정상 범위 `2`-비정상 범위|\n",
    "|ldl|저밀도 콜레스테롤||\n",
    "|hba1c|당화 혈색소||\n",
    "|hba1c_65|당화 혈색소 6.5기준| `1` - 6.5 미만(정상) `2` - 6.5 초과|\n",
    "|crp|C반응 단백질|`1`-0.3 미만(정상) `2`-0.3 초과|\n",
    "|hs_crp|C반응 단백질||\n",
    "|cea|암 배아 항원(항암제)||\n",
    "|ft4|갑상선 검사 수치||\n",
    "|tsh|갑상선 자극 호르몬||\n",
    "|ft3|갑상선 검사 수치||\n",
    "|insulin|인슐린||\n",
    "|homa_ir|인슐린 저항성 검사||\n",
    "|ferritin|페리틴(혈당 속 철분 수치)||\n",
    "|ob_yn|잠혈 검사횟수|`0`-안 함 `1`-1회 `2`-2회 `3`-3회 `7`-3회 이상|\n",
    "|ob_rt|잠혈 수치| | \n",
    "|meta_yn|전이 여부|`1`-전이O `2`-전이X|\n",
    "|mets_yn|대사성 비정상 세포|`1`-없음 `2`-있음|\n",
    "|meta_n|대사성 비정상 세포 개수|`0`-0개 `1`-1개 `2`-2개 `3`-3개 `4`-4개 `5`-5개|\n",
    "|obe_meta|비만, 대사성 비정상 세포|`1`-둘 다 없음 `2`-비만만 있음 `3`-대사성 비정상 세포만 있음 `4`-둘 다 있음|\n",
    "|alc_yn|음주 여부|`0`-안마심 `1`-과거에는 안마셨으나 현재 마심 `2`-마심|\n",
    "|alc_freq|음주 빈도 (일주일에 얼마나)|`0`-전혀 안 마심|\n",
    "|smoke|흡연|`0`-비흡연 `1`-가끔 흡연 `2`-흡연|\n",
    "|dm|당뇨|`0`-정상 `1`-당뇨|\n",
    "|htn|고혈압|`0`-정상 `1`-고혈압|\n",
    "|sbp|수축기 혈압| |\n",
    "|dbp|이완기 혈압| |\n",
    "|bp_13580|혈압 정상 범위 기준|`0`-135~80 범위 내(정상) `1`-135~80 범위 밖|\n",
    "|ca_hx|과거 암이력|`0`-없음 `1`-있음|\n",
    "|crc_hx|장 근처 암이력||\n",
    "|crc_fhx|결장 직장암 가족력|`0`-없음 `1`-있음|\n",
    "|exercise|운동여부| `0`-안한다 `1`-한다|\n",
    "|analge |진통제 |`0`-섭취하지 않음|\n",
    "|acrn_yn|**대장암 발병 유무 (Y/N)**|`0`-정상 `1`-대장암|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check `Null` Value\n",
    "\n",
    "결측치나 이상치를 처리하는 방법은 크게 3가지이다.\n",
    "1. 그냥 무시 -> 상당히 문제가 발생할 것을 각오\n",
    "2. 해당 값이 존재하는 행을 제거 - df.dropna()\n",
    "3. 다른 값으로 대치 - df.fillna()  \n",
    "\n",
    "출처 : [Missing Data (NA,NaN) 처리](https://ordo.tistory.com/59)\n",
    "\n",
    "일단 변수별로 결측치가 얼마나 존재하는지 파악해보자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 결측치 파악\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#결측치가 있는 행 비율 보기\n",
    "missing_df = data.isnull().sum().reset_index()\n",
    "missing_df.columns = ['column', 'count']\n",
    "missing_df['ratio'] = missing_df['count'] / len(data)\n",
    "missing_df.loc[missing_df['ratio'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 결측치 시각화\n",
    "msno.matrix(data.iloc[:,:30], figsize=(18,6));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "msno.matrix(data.iloc[:,30:], figsize=(18,6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check `Dependent` Variable\n",
    "기본적으로 종속변수의 분포를 살펴봅니다. `종속변수`란 다른 변수들의 관계를 주로 추론하고, `최종적으로는 예측하고자 하는 변수`입니다.  \n",
    "출처: https://3months.tistory.com/325 [Deep Play]\n",
    "\n",
    "아래 보는 바와 같이, 대장암 걸리지 않은 정상인의 수가 월등히 많음을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 대장암 발병 환자의 수\n",
    "data['acrn_yn'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 데이터 중 대장암 비율\n",
    "data['acrn_yn'].value_counts() / len(data) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check `Categorical` Variable\n",
    "종속변수의 분포를 파악했으면, 다음으로는 명목형 변수에 대한 탐색이 이루어져한다.\n",
    "\n",
    "해당 데이터의 명목형 변수에는 총 17개의 변수가 존재한다. 5개 변수에 대해서만 나열해보자면 다음과 같다\n",
    "1. 성별( `sex` )\n",
    "2. 비만도( `obesity` )\n",
    "3. 복부비만( `abd_obe` )\n",
    "4. 혈당 100 기준( `glu_100` )\n",
    "5. 트리 글리세라이드 150 기준( `tg_150` )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 범주형 데이터와 연속형 데이터 분류\n",
    "category = ['sex', 'obesity','abd_obe', 'glu_100', 'tg_150', 'hdl_abnl', 'hba1c_65', \n",
    "         'crp', 'ob_yn', 'meta_yn', 'mets_yn', 'meta_n', 'obe_meta', 'alc_yn', \n",
    "          'alc_freq', 'smoke', 'dm', 'htn', 'bp_13580', 'ca_hx', 'crc_fhx', \n",
    "         'exercise', 'analge', 'acrn_yn']\n",
    "continuous = [i for i in data.columns if i not in category]\n",
    "category.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=5, ncols=4)\n",
    "fig.set_size_inches(15, 15)\n",
    "\n",
    "nominal_variables = ['sex','obesity','abd_obe','glu_100',\n",
    "                    'tg_150','hdl_abnl','hba1c_65','crp',\n",
    "                    'meta_yn','mets_yn','alc_yn','alc_freq',\n",
    "                    'smoke','dm','bp_13580','crc_fhx',\n",
    "                    'exercise']\n",
    "\n",
    "sns.barplot(data['sex'], data['acrn_yn'], ax=axes[0,0]);\n",
    "sns.barplot(data[\"obesity\"], data[\"acrn_yn\"], ax=axes[0,1]);\n",
    "sns.barplot(data[\"abd_obe\"], data[\"acrn_yn\"], ax=axes[0,2]);\n",
    "sns.barplot(data[\"glu_100\"], data[\"acrn_yn\"], ax=axes[0,3]);\n",
    "sns.barplot(data[\"tg_150\"], data[\"acrn_yn\"], ax=axes[1,0]);\n",
    "sns.barplot(data[\"hdl_abnl\"], data[\"acrn_yn\"], ax=axes[1,1]);\n",
    "sns.barplot(data[\"hba1c_65\"], data[\"acrn_yn\"], ax=axes[1,2]);\n",
    "sns.barplot(data[\"crp\"], data[\"acrn_yn\"], ax=axes[1,3]);\n",
    "sns.barplot(data[\"meta_yn\"], data[\"acrn_yn\"], ax=axes[2,0]);\n",
    "sns.barplot(data[\"mets_yn\"], data[\"acrn_yn\"], ax=axes[2,1]);\n",
    "sns.barplot(data[\"alc_yn\"], data[\"acrn_yn\"], ax=axes[2,2]);\n",
    "sns.barplot(data[\"alc_freq\"], data[\"acrn_yn\"], ax=axes[2,3]);\n",
    "sns.barplot(data[\"smoke\"], data[\"acrn_yn\"], ax=axes[3,0]);\n",
    "sns.barplot(data[\"dm\"], data[\"acrn_yn\"], ax=axes[3,1]);\n",
    "sns.barplot(data[\"bp_13580\"], data[\"acrn_yn\"], ax=axes[3,2]);\n",
    "sns.barplot(data[\"crc_fhx\"], data[\"acrn_yn\"], ax=axes[3,3]);\n",
    "sns.barplot(data[\"exercise\"], data[\"acrn_yn\"], ax=axes[4,0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#범주형 단변수를 조금 더 자세히 살펴보자\n",
    "for col in category: \n",
    "    data[col].value_counts().plot(kind='bar') \n",
    "    plt.title(col) \n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#그룹별 비율\n",
    "for col in category:\n",
    "     res_df = data.groupby([col,'acrn_yn'])[col].count().unstack('acrn_yn') / [69376, 960]\n",
    "     res_df.plot(kind='bar')\n",
    "     plt.title(col)\n",
    "     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#연속형 단변수\n",
    "for col in continuous:\n",
    "    sns.distplot(data.loc[data[col].notnull(), col])\n",
    "    plt.title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 여기서 `crp`, `alc_yn`, `bp_1350`, `exercise`가 대장암 발병에는 큰 영향을 주지 않음을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check `Continuous` Variable\n",
    "다음으로는 **연속형 데이터**에 대해서 알아보자\n",
    "\n",
    "데이터에 존재하는 수많은 특징을 모두 시각화할 수 없지만, 일부 데이터에 대해서 시각화를 해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  나이(Age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.FacetGrid(data, hue='acrn_yn', aspect=4)\n",
    "grid.map(sns.kdeplot, 'age', shade=True)\n",
    "grid.add_legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 신장(height)\n",
    "질병 발생의 원인으로는 키(height)는 해당되지 않은 것으로 보인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.FacetGrid(data, hue='acrn_yn',aspect=4)\n",
    "grid.map(sns.kdeplot, 'height', shade=True)\n",
    "grid.add_legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 체중(weight)\n",
    "\n",
    "질병 발생의 원인으로는 몸무게(`weight`)은 해당되지 않은 것으로 보인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.FacetGrid(data, hue='acrn_yn',aspect=4)\n",
    "grid.map(sns.kdeplot, 'weight', shade=True)\n",
    "grid.add_legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 백혈구(wbc)\n",
    "질병 발생의 원인으로는 몸무게(weight)은 해당되지 않은 것으로 보인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.FacetGrid(data, hue='acrn_yn',aspect=4)\n",
    "grid.map(sns.kdeplot, 'wbc', shade=True)\n",
    "grid.set(xlim=(0, 20))\n",
    "grid.add_legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 헤모글로빈(hb)\n",
    "질병 발생의 원인으로는 몸무게(weight)은 해당되지 않은 것으로 보인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid = sns.FacetGrid(data, hue='acrn_yn',aspect=4)\n",
    "grid.map(sns.kdeplot, 'hb', shade=True)\n",
    "grid.add_legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 적혈구 크기 분포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.FacetGrid(data, hue='acrn_yn',aspect=4)\n",
    "grid.map(sns.kdeplot, 'rdw', shade=True)\n",
    "grid.add_legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 혈소판(plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.FacetGrid(data, hue='acrn_yn',aspect=4)\n",
    "grid.map(sns.kdeplot, 'plt', shade=True)\n",
    "grid.add_legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 프로틴(prot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.FacetGrid(data, hue='acrn_yn',aspect=4) #스무스하게 속성 추가\n",
    "grid.map(sns.kdeplot, 'prot', shade=True)\n",
    "grid.add_legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 알부민(단백질의 일종)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.FacetGrid(data, hue='acrn_yn',aspect=4)\n",
    "grid.map(sns.kdeplot, 'alb', shade=True)\n",
    "grid.add_legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 글로불린(glob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.FacetGrid(data, hue='acrn_yn',aspect=4)\n",
    "grid.map(sns.kdeplot, 'glob', shade=True)\n",
    "grid.add_legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation between `Nomial` and `Numerical` Variable\n",
    "\n",
    "### feature들 간의 correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c_mat=data.corr()\n",
    "fig, ax=plt.subplots(figsize=(12,9))\n",
    "sns.heatmap(c_mat,square=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### acrn_yn과 correlation이 높은 10개의 feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k=10\n",
    "cols=c_mat.nlargest(k,'acrn_yn')['acrn_yn'].index\n",
    "cm=np.corrcoef(data[cols].values.T)\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### correlation이 0.7보다 큰 feature pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def redundant_pair(df):\n",
    "    drop_pairs=set()\n",
    "    cols=df.columns\n",
    "    for i in range(df.shape[1]):\n",
    "        for j in range(i+1):\n",
    "            drop_pairs.add((cols[i],cols[j]))\n",
    "    return drop_pairs\n",
    "\n",
    "def high_corr_pair(df,threshold):\n",
    "    corr=df.corr().abs().unstack()\n",
    "    drops=redundant_pair(df)\n",
    "    corr=corr.drop(drops)\n",
    "    return corr[corr.abs()>threshold].index.tolist()\n",
    "\n",
    "corr_pairs=high_corr_pair(data,0.7)\n",
    "corr_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 비례하는 상관 관계 파악\n",
    "- weight, bmi, wt_cirt\n",
    "- rbc, hb, hct\n",
    "- t_chol, ldl\n",
    "- sbp, dbp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i,pair in enumerate(corr_pairs):\n",
    "    plt.figure(figsize=(16,20))\n",
    "    plt.subplot(7,6,i+1)\n",
    "    plt.scatter(data[pair[0]],data[pair[1]]);\n",
    "    plt.xlabel(pair[0])\n",
    "    plt.ylabel(pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dist_similars='abd_obe,alc_freq,ca_hx,cea,crc_fhx,crp,dm,hba1c,hba1c_65,hdl_abnl,homa_ir,hs_crp,insulin,ldh,tg,tsh,wbc'\n",
    "dist_similars=dist_similars.split(',')\n",
    "\n",
    "for i,col in enumerate(dist_similars):\n",
    "    plt.figure(figsize=(16,20))\n",
    "    plt.subplot(5,4,i+1)\n",
    "    plt.scatter(data[col],data['acrn_yn'])\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('acrn_yn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing value 처리\n",
    "- 연속형 - 평균값으로 대체\n",
    "- 범주형 - 최빈값으로 대체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# null 인 feature들의 최빈값\n",
    "null_cols=['height','hct','rdw','plt','ca','hs_crp','ft3','ferritin','ob_yn','alc_freq','sbp','dbp']\n",
    "\n",
    "for col in null_cols:\n",
    "    print('{} - most_freq: {}'.format(col,data[col].mode()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mean_cols=['height','hct','plt','hs_crp','ft3','ferritin']\n",
    "fill_mfv_cols=['ob_yn','alc_freq','rdw','ca','sbp','dbp']\n",
    "\n",
    "# map(lambda x: data[x].fillna(data[x].mean(),inplace=True),fill_mean_cols)\n",
    "# map(lambda x: data[x].fillna(data[x].mode()[0],inplace=True),fill_mfv_cols)\n",
    "\n",
    "for col in fill_mean_cols:\n",
    "    data[col].fillna(data[col].mean(),inplace=True)\n",
    "    \n",
    "for col in fill_mfv_cols:\n",
    "    data[col].fillna(data[col].mode()[0],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[fill_mean_cols].isnull().sum())\n",
    "print(data[fill_mfv_cols].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scaling\n",
    "continuous 변수 평균을 0으로 만들어 주기 위해 Standardization를 수행하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaling_features = [col for col in continuous if col in data.columns]\n",
    "scaling_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = data.loc[:, scaling_features].values\n",
    "x = StandardScaler().fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data = data[:]\n",
    "i = 0\n",
    "for col in scaling_features:\n",
    "    scaled_data[col] = x[:, i]\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터를 다시 한번 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scaled_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Feature Engineering&Selection\n",
    "제공받은 자료가 이미 깔끔하게 정리돼었다.  \n",
    "그래도 변수를 좀 더 정리하자면"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## drop 시킬 후보\n",
    "- no : 의미 없음\n",
    "- studyid : 의미 없음\n",
    "- crc_hx : 너무 한 값에 몰려 있음\n",
    "- ob_rt : null 값이 너무 많음\n",
    "- analge : null 값이 너무 많음\n",
    "- exercise : 분포가 binary로 큰 차이가 없음\n",
    "\n",
    "## 강하게 비슷한 분포\n",
    "- abd_obe\n",
    "- alc_freq\n",
    "- ca_hx\n",
    "- cea\n",
    "- crc_fhx\n",
    "- crp\n",
    "- dm\n",
    "- hbalc\n",
    "- hbalc_65\n",
    "- hdl_abnl\n",
    "- homa_ir\n",
    "- hs_crp\n",
    "- insulin\n",
    "- ldh\n",
    "- tg\n",
    "- tsh\n",
    "- wbc\n",
    "\n",
    "## 약하게 비슷한 분포(특정 범위를 그룹으로 묶어야 하는 경우이거나 split purity가 좀 낮은 경우)\n",
    "- bmi\n",
    "- ca\n",
    "- ferritin\n",
    "- ft3\n",
    "- ft\n",
    "- alc_yn(peak가 높은 두 값을 한 그룹으로 합치면 분포 비슷함)\n",
    "- glob\n",
    "- glu_100(split purity가 약함)\n",
    "- glucose\n",
    "- htn\n",
    "- ldl\n",
    "- meta_n(특정 값 이상으로 그룹 묶으면 분포 비슷)\n",
    "- mets_yn\n",
    "- ob_yn\n",
    "- plt\n",
    "- rdw\n",
    "- t_chol\n",
    "\n",
    "## 애매한 것들\n",
    "- dbp\n",
    "- hb(분포가 너무 넓음)\n",
    "- hct(분포가 너무 넓음)\n",
    "- hdl(분포가 넓음)\n",
    "- height(분포가 너무 넓음)\n",
    "- meta_n(분포값이 비례 관계에 있음)\n",
    "- meta_yn\n",
    "- obe_meta\n",
    "- obesity\n",
    "- prot\n",
    "- rbc\n",
    "- sdp\n",
    "- sex\n",
    "- smoke\n",
    "- weight\n",
    "- wt_circ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature select\n",
    "의미 없는 변수\n",
    "- no\n",
    "- studyid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_drop = set(['no','studyid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "null 값이 10000개 이상인 변수\n",
    "- ob_rt \n",
    "- crc_hx\n",
    "- analge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_drop.update(['ob_rt','crc_hx','analge'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "큰 상관관계 없는 변수\n",
    "- crp\n",
    "- alc_yn\n",
    "- bp_13580\n",
    "- ca_hx (13개 데이터를 제외하고는 모두 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_drop.update(['crp','alc_yn','bp_13580', 'ca_hx'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "분포가 비슷한 변수\n",
    "- height\n",
    "- weight\n",
    "- wbc\n",
    "- hb\n",
    "- rdw\n",
    "- plt\n",
    "- prot\n",
    "- alb\n",
    "- glob\n",
    "- exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_drop.update(['height','weight','wbc','hb','rdw','plt','prot','alb','glob','exercise'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "발병유무와 관계 없이 분포가 비슷한 변수\n",
    "- hdl_abnl\n",
    "- alc_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_drop.update(['hdl_abnl','alc_freq'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "null 값이 1000개 이상인 변수 > drop(?)\n",
    "- ca\n",
    "- hs_crp\n",
    "- ft3\n",
    "- ob_yn\n",
    "- ob_rt\n",
    "- crc_hx\n",
    "- analge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_drop.update(['ca','hs_crp','ft3','ob_yn','ob_rt','crc_hx','analge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지워야할 feature의 수\n",
    "feature_drop = list(feature_drop)\n",
    "len(feature_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = scaled_data.drop(feature_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델링을 하기 앞서, 가지고 있는 데이터셋을 Train과 Test로 나누기 위한 작업해야합니다. 그걸 위해서 교차검증( Cross Validation )방법을 이용하겠습니다. 그리고 나서는 아래와 같이 머신러닝에서 사용되는 기본적인 모델에 우리가 가지고 있는 학습 데이터를 넣어볼 생각입니다.\n",
    "\n",
    "- 로지스틱 회귀분석 ( Logistic Regression )\n",
    "- 최근접 이웃법 ( K-Nearest Neighbor Algorithm ,kNN )\n",
    "- 의사결정나무( Decision Tree )\n",
    "- 랜덤포레스트 ( Random Forest )\n",
    "- 나이브 베이즈 ( Naive Bayes ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Test 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#훈련세트, 테스트세트 나누기\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(final_data, test_size = 0.3)\n",
    "type(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train['acrn_yn']\n",
    "train = train.drop(['acrn_yn'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 교차검증 ( Cross Validation )\n",
    "[DeepPlay - 딥러닝 모델의 교차검증 (Cross Validation)](https://3months.tistory.com/321)\n",
    "\n",
    "### 교차 검증이란?\n",
    "통계학에서 모델을 **평가**하는 방법입니다. \n",
    "\n",
    "모델을 평가하기 위해 기본적인 방법은 바로 **Train Set과 Test Set으로 분리**하여 Train Set으로 모델의 계수를 추정하고 Test Set으로 성능을 평가하는 것인데요.\n",
    "\n",
    "**교차검증**은 이러한 기본적인 작업의 문제점을 보완하기 위해서 쓰입니다. 이 문제점이란 **바로 Test Set의 크기가 작은 경우 Test Set에 대한 성능 평가의 신뢰성이 떨어지게 된다는 것**입니다. Test Set을 어떻게 잡느냐에 따라 성능이 아주 상이하게 나온다면, 우연에 의한 효과로 인해 모델 평가 지표에 **편향이 생기게** 되겠죠. \n",
    "\n",
    "이를 해결하기 위해 교차 검증은 **`모든 데이터가 최소 한 번은 Test Set으로 쓰이도록`** 합니다. 아래의 그림을 보면,데이터를 5개로 쪼개 매번 Test Set을 바꿔나가는 것을 볼 수 있습니다.\n",
    "\n",
    "![](https://t1.daumcdn.net/cfile/tistory/990DD2465B72F1491E)\n",
    "\n",
    "첫 번째 Iteration에서는 BCDE를 Train Set으로, A를 Test Set으로 설정한 후, 성능을 평가합니다.  \n",
    "두 번째 Iteration에서는 ACDE를 Train Set으로, B를 Test Set으로하여 성능을 평가합니다.\n",
    "\n",
    "그러면 총 5개의 성능 평가지표가 생기게 되는데, 보통 이 값들을 평균을 내어 모델의 성능을 평가하게 됩니다.  \n",
    "(아래 데이터는 모두 사실은 Train Data입니다. Iteration이라는 상황안에서만 Test Set이 되는 것입니다.)\n",
    "\n",
    "(cf. **여기서 validation set은 없습니다.** 성능 평가를 할 때, `validation set을 사용하냐 아니면 cross validation을 사용하냐, 둘 중하나를 선택하는 걸로 이해`하시면 됩니다. 데이터의 수가 적을 때는 시간은 오래걸리지만 cross validation을 하는 것이 일반적입니다. )\n",
    "\n",
    "### 교차검증을 통해 달성할 수 있는 성능 평가의 목적\n",
    "\n",
    "모델 성능 평가는 보통 2개의 목적이 있습니다.\n",
    "1. Unseen Data에 대한 성능을 예측하기 위해\n",
    "2. 더 좋은 모델을 선택하기 위해 (혹은 Hyperparameter Tuning) 입니다.\n",
    "\n",
    "교차 검증은 1,2를 달성하기 위한 좋은 방법입니다. \n",
    "\n",
    "파이썬에서 교차검증을 하는 방법은 여러가지가 있지만 **scikit-learn이 많이 사용**됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation (K-fold)\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "k_fold = KFold(n_splits=10, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 로지스틱 회귀 ( Logistic Regression )\n",
    "\n",
    "[Deep Play - 로지스틱 회귀분석의 원리와 장점](https://3months.tistory.com/327)\n",
    "\n",
    "로지스틱 회귀분석은 **반응변수가 1 또는 0인 이진형 변수에서 쓰이는 회귀분석 방법**입니다. 종속변수에 로짓변환을 실시하기 때문에 로지스틱 회귀분석이라고 불립니다. 로지스틱 회귀분석의 좋은점은 우선 계수가 Log Odds ratio가 되기 때문에 해석이 매우 편리하고, case-control과 같이 반응 변수에 따라 샘플링된 데이터에 대해서 편의(bias)가 없는 타당한 계수 추정치를 계산할 수 있다는 것입니다. 이 부분에 초점을 맞추어 로지스틱 회귀분석에 대해 간단히 정리해보려고합니다. \n",
    "\n",
    "![](https://t1.daumcdn.net/cfile/tistory/995162335B88F09606)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "scoring = 'accuracy'\n",
    "score = cross_val_score(clf, train, target, cv=k_fold, n_jobs=1, scoring=scoring)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Score\n",
    "round(np.mean(score)*100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN ( K-Nearest Neighbor )\n",
    "[ Minsuk Heo Youtube - kNN ](https://www.youtube.com/watch?v=CyuI2F_wJWw)  \n",
    "[ 곰가드 - K-최근접 이웃 (K-Nearest Neighbor) ](https://gomguard.tistory.com/51)\n",
    "\n",
    "최근접 이웃법은 한마디로 **유유상종**이라고 할 수 있습니다. 새로운 데이터를 입력 받았을 때 **가장 가까이 있는 것이 무엇이냐를 중심으로** 새로운 데이터의 종류를 정해주는 알고리즘입니다.\n",
    "![](https://t1.daumcdn.net/cfile/tistory/99631D335A165F182D)\n",
    "\n",
    "단순히 주변에 무엇이 가장 가까이 있는가를 보는 것이 아니라 **주변에 있는 몇개의 것들을 같이 봐서 가장 많은 것을 골라내는 방식을 사용**하게 됩니다. 이 방식을 KNN 이라고 부릅니다. KNN 에서 K 는 주변의 개수를 의미합니다.\n",
    "![](https://t1.daumcdn.net/cfile/tistory/9929D1335A16627203)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier(n_neighbors = 13)\n",
    "scoring = 'accuracy'\n",
    "score = cross_val_score(clf, train, target, cv=k_fold, n_jobs=1, scoring=scoring)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kNN Score\n",
    "round(np.mean(score)*100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 의사결정나무 ( Decision Tree)\n",
    "[ratsgo's blog - 의사결정나무](https://ratsgo.github.io/machine%20learning/2017/03/26/tree/)\n",
    "\n",
    "이번 포스팅에선 한번에 하나씩의 설명변수를 사용하여 예측 가능한 규칙들의 집합을 생성하는 알고리즘인 의사결정나무(Decision Tree)에 대해 다뤄보도록 하겠습니다. \n",
    "\n",
    "### 모델소개\n",
    "의사결정나무는 **데이터를 분석하여 이들 사이에 존재하는 패턴을 예측 가능한 규칙들의 조합으로 나타내며**, 그 모양이 ‘나무’와 같다고 해서 의사결정나무라 불립니다. 질문을 던져서 대상을 좁혀나가는 **‘스무고개’ 놀이와 비슷**한 개념입니다.\n",
    "![](http://i.imgur.com/ZKDnzOB.png)\n",
    "위 예시는 운동경기가 열렸다면 PLAY=1, 그렇지 않으면 PLAY=0으로 하는 이진분류(binary classification) 문제입니다. 모든 사례를 조사해 그림으로 도시하면 위와 같은 그림이 되는 것입니다. 그림을 한번 해석해볼까요? 날씨가 맑고(sunny), 습도(humidity)가 70 이하인 날엔 경기가 열렸습니다. 해당 조건에 맞는 데이터들이 ‘경기가 열렸다(play 2건)’고 말하고 있기 때문입니다. 반대로 비가 오고(rain) 바람이 부는(windy) 날엔 경기가 열리지 않았습니다.\n",
    "\n",
    "의사결정나무는 **분류(classification)와 회귀(regression) 모두 가능**합니다. 범주나 연속형 수치 모두 예측할 수 있다는 말입니다. 의사결정나무의 범주예측, 즉 분류 과정은 이렇습니다. 새로운 데이터가 특정 terminal node에 속한다는 정보를 확인한 뒤 해당 terminal node에서 가장 빈도가 높은 범주에 새로운 데이터를 분류하게 됩니다. 운동경기 예시를 기준으로 말씀드리면 날씨는 맑은데 습도가 70을 넘는 날은 경기가 열리지 않을 거라고 예측합니다.\n",
    "\n",
    "### 불순도/불확실성\n",
    "의사결정나무는 한번 분기 때마다 변수 영역을 두 개로 구분하는 모델이라고 설명을 드렸는데요, 그렇다면 대체 어떤 기준으로 영역을 나누는 걸까요? 이 글에서는 타겟변수(Y)가 범주형 변수인 분류나무를 기준으로 설명하겠습니다.\n",
    "\n",
    "결론부터 말씀드리면 분류나무는 구분 뒤 각 영역의 순도(homogeneity)가 증가, 불순도(impurity) 혹은 불확실성(uncertainty)이 최대한 감소하도록 하는 방향으로 학습을 진행합니다. 순도가 증가/불확실성이 감소하는 걸 두고 정보이론에서는 정보획득(information gain)이라고 합니다. 이번 챕터에서는 어떤 데이터가 균일한 정도를 나타내는 지표, 즉 순도를 계산하는 3가지 방식인 **엔트로피, 지니계수, 오분류오차**가 있습니다.\n",
    "![](http://i.imgur.com/heIgkif.png)\n",
    "\n",
    "### 모델 학습\n",
    "의사결정나무의 학습 과정은 입력 변수 영역을 두 개로 구분하는 **재귀적 분기(recursive partitioning)** 와 너무 자세하게 구분된 영역을 통합하는 **가지치기(pruning)** 두 가지 과정으로 나뉩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "scoring = 'accuracy'\n",
    "score = cross_val_score(clf, train, target, cv=k_fold, n_jobs=1, scoring=scoring)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree Score\n",
    "round(np.mean(score)*100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 랜덤포레스트 ( Ramdom Forest )\n",
    "의사결정나무는 **계산복잡성 대비 높은 예측 성능을 내는 것으로 정평**이 나 있습니다. 아울러 **변수 단위로 설명력을 지닌다는 강점**을 가지고 있습니다. 다만 의사결정나무는 **결정경계(decision boundary)가 데이터 축에 수직이어서 비선형(non-linear) 데이터 분류엔 적합하지 않습니다.**\n",
    "\n",
    "이같은 문제를 극복하기 위해 등장한 모델이 바로 **랜덤포레스트**인데요, **같은 데이터에 대해 의사결정나무를 여러 개 만들어 그 결과를 종합**해 예측 성능을 높이는 기법입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=13)\n",
    "scoring = 'accuracy'\n",
    "score = cross_val_score(clf, train, target, cv=k_fold, n_jobs=1, scoring=scoring)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Score\n",
    "round(np.mean(score)*100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 나이브 베이즈 ( Naive Bayes ) \n",
    "[ratsgo's blog](https://ratsgo.github.io/machine%20learning/2017/05/18/naive/)  \n",
    "[딥 러닝을 이용한 자연어 처리 입문 ](https://wikidocs.net/22892)\n",
    "\n",
    "### 베이즈의 정리(Bayes' theorem)를 이용한 분류 메커니즘\n",
    "나이브 베이즈 분류기를 이해하기 위해서는 우선 베이즈의 정리(Bayes' theorem)를 이해할 필요가 있습니다. 베이즈 정리는 조건부 확률을 계산하는 방법 중 하나입니다.\n",
    "\n",
    "P(A)가 A가 일어날 확률, P(B)가 B가 일어날 확률, P(B|A)가 A가 일어나고나서 B가 일어날 확률, P(A|B)가 B가 일어나고나서 A가 일어날 확률이라고 해봅시다. 이 때 P(B|A)를 쉽게 구할 수 있는 상황이라면, 아래와 같은 식을 통해 P(A|B) 또한 구할 수 있습니다.\n",
    "![](https://wikidocs.net/images/page/22892/%EB%B2%A0%EC%9D%B4%EC%A6%88%EC%A0%95%EB%A6%AC.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "scoring = 'accuracy'\n",
    "score = cross_val_score(clf, train, target, cv=k_fold, n_jobs=1, scoring=scoring)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Score\n",
    "round(np.mean(score)*100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling\n",
    "\n",
    "데이터 불균형을 해결하기 위한 방법 중 하나\n",
    "불균형이 심한 데이터를 모델에 그대로 넣게 되면 터무니없이 높은 정확도가 나올 수 있다.\n",
    "\n",
    "종류\n",
    "- Random Down-sampling (Under-sampling)\n",
    "    - train set에서 major 집단(acrn_yn=0)을 Down-sampling해 minor 집단(acrn_yn=1)과 균형을 맞춰준다.\n",
    "    - 장점 : 전체 샘플 수를 줄임으로써 저장 문제, 실행 속도 문제를 개선할 수 있다.\n",
    "    - 단점 : 중요한 정보를 누락할 수 있다. 정확한 대표성을 가지지 못하면 부정확한 결과가 나올 수 있다.\n",
    "\n",
    "- Random Up-sampling (Over-sampling)\n",
    "    - train set에서 minor 집단(acrn_yn=1)을 Up-sampling해 major 집단(acrn_yn=0)과 균형을 맞춰준다.\n",
    "    - 장점 : Down-sampling보다 정보 손실이 적어 성능이 좋다.\n",
    "    - 단점 : 소수 집단을 복사하기 때문에 overfitting 가능성이 있다.\n",
    "    \n",
    "주의할 점\n",
    "- 종속변수의 비율을 유지한 채 train, test로 분할한 후, train 데이터에 대해서만 resampling!\n",
    "- 너무 적은 데이터에 대해 과도한 sampling 파라미터를 주게 되면 새로운 데이터를 분석하는 성능이 떨어진다\n",
    "\n",
    "[출처] https://sherry-data.tistory.com/22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y 비율을 유지하면서 train, test 분할\n",
    "X = final_data.iloc[:, :-1]\n",
    "y = final_data['acrn_yn'] #target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3,\n",
    "                                                   random_state = 12, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"train set : \", X_train.shape, y_train.shape)\n",
    "print(\"test set : \", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "ros_down = RandomUnderSampler(random_state=0)\n",
    "X_resampled_d, y_resampled_d = ros_down.fit_resample(X_train, y_train)\n",
    "\n",
    "from collections import Counter\n",
    "print(sorted(Counter(y_resampled_d).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_resampled_d.shape, y_resampled_d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros_up = RandomOverSampler(random_state=0)\n",
    "X_resampled_u, y_resampled_u = ros_up.fit_resample(X_train, y_train)\n",
    "\n",
    "print(sorted(Counter(y_resampled_u).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X_resampled_u.shape, y_resampled_u.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 앙상블(Bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf1 = BaggingClassifier(\n",
    "    DecisionTreeClassifier(random_state=42), n_estimators=500, #500개 결정 트리 분류기\n",
    "    max_samples=100, bootstrap=True, n_jobs=-1, random_state=42)\n",
    "bag_clf1.fit(X_resampled_d, y_resampled_d)\n",
    "y_pred1 = bag_clf1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"정확도 : \", accuracy_score(y_test, y_pred1)*100)\n",
    "print(\"정밀도 : \", precision_score(y_test, y_pred1)*100)\n",
    "print(\"재현율 : \", recall_score(y_test, y_pred1)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf2 = BaggingClassifier(\n",
    "    DecisionTreeClassifier(random_state=42), n_estimators=500, #500개 결정 트리 분류기\n",
    "    max_samples=100, bootstrap=True, n_jobs=-1, random_state=42)\n",
    "bag_clf2.fit(X_resampled_u, y_resampled_u)\n",
    "y_pred2 = bag_clf2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"정확도 : \", accuracy_score(y_test, y_pred2)*100)\n",
    "print(\"정밀도 : \", precision_score(y_test, y_pred2)*100)\n",
    "print(\"재현율 : \", recall_score(y_test, y_pred2)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA(Principal Component Analysis)\n",
    "- 원래 데이터와 비슷한 분포를 유지하면서 고차원의 데이터를 저차원으로 만드는 차원 축소방법\n",
    "- ‘잠재변수와 데이터가 선형적인 관계로 연결되어 있다.는 가정 필요\n",
    "- 데이터를 잘 설명해주는 주성분 벡터는 데이터의 분산을 가장 크게하는 벡터\n",
    "\n",
    "PCA를 사용해 주성분 top 20을 선별해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Up-sampling data 사용\n",
    "principal_Components = pca.fit_transform(X_resampled_u)\n",
    "principal_Df = pd.DataFrame(data = principal_Components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca_Df = pd.concat([principal_Df, data[['acrn_yn']]], axis = 1)\n",
    "pca_Df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추출된 주성분은 pipeline 함수를 이용해 아래 xgboosting 모델에서 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/boosting-algorithm-xgboost-4d9ec0207d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=final_data.iloc[:,:-1]\n",
    "y=final_data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler as down_sampler\n",
    "from imblearn.over_sampling import RandomOverSampler as up_sampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "down=down_sampler(random_state=42)\n",
    "up=up_sampler(random_state=42)\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,stratify=y)\n",
    "\n",
    "X_up_train,y_up_train=up.fit_resample(X_train,y_train)\n",
    "X_down_train,y_down_train=down.fit_resample(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('UP sampling : 1-{} 0-{}'.format(X_up_train[X_up_train[:,-1]==1].shape,X_up_train[X_up_train[:,-1]==0].shape))\n",
    "print('Down sampling : 1-{} 0-{}'.format(X_down_train[X_down_train[:,-1]==1].shape,X_down_train[X_down_train[:,-1]==0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, accuracy_score, precision_score, recall_score\n",
    "\n",
    "xgb_=XGBClassifier()\n",
    "xgb1=XGBClassifier()\n",
    "xgb2=XGBClassifier()\n",
    "\n",
    "\n",
    "print('-----with no sampling-----')\n",
    "xgb_.fit(X_train,y_train)\n",
    "y_pred=xgb_.predict(X_test)\n",
    "print('Train acc : {:.2%}\\nTest acc : {:.2%}\\n'.format(xgb_.score(X_train,y_train),accuracy_score(y_test,y_pred)))\n",
    "\n",
    "print('-----with UP-sampling-----')\n",
    "X_up_train=pd.DataFrame(X_up_train,columns=X.columns)\n",
    "xgb1.fit(X_up_train,y_up_train)\n",
    "y_up_pred=xgb1.predict(X_test)\n",
    "print('Train acc : {:.2%}\\nTest acc : {:.2%}\\n'.format(xgb1.score(X_up_train,y_up_train),accuracy_score(y_test,y_up_pred)))\n",
    "\n",
    "print('-----with DOWN-sampling-----')\n",
    "X_down_train=pd.DataFrame(X_down_train,columns=X.columns)\n",
    "xgb2.fit(X_down_train,y_down_train)\n",
    "y_down_pred=xgb2.predict(X_test)\n",
    "print('Train acc : {:.2%}\\nTest acc : {:.2%}\\n'.format(xgb2.score(X_down_train,y_down_train),accuracy_score(y_test,y_down_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=> generalization 측면에서 upsampling이 우월함\n",
    "\n",
    "=> sampling을 안했을 때 정확도가 훨씬 높지만 이것은 acrn_rn = 0인 데이터가 압도적으로 많아서 accuracy만 높은 것이다.\n",
    "\n",
    "=> 이렇게 imbalance한 데이터에 대해서는 precision / recall이 더 적절함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_=xgb_.predict_proba(X_test)\n",
    "proba_up=xgb1.predict_proba(X_test)\n",
    "proba_down=xgb2.predict_proba(X_test)\n",
    "\n",
    "proba_,proba_up,proba_down=proba_[:,1],proba_up[:,1],proba_down[:,1]\n",
    "\n",
    "\n",
    "pre,rec,thr=precision_recall_curve(y_test,proba_)\n",
    "plt.title('with no sampling')\n",
    "plt.plot(thr, pre[:-1], 'b--', label='precision')\n",
    "plt.plot(thr, rec[:-1], 'g--', label = 'recall')\n",
    "plt.xlabel('Threshold')\n",
    "plt.legend(loc='upper left')\n",
    "plt.ylim([0,1])\n",
    "plt.show()\n",
    "print('-----with no sampling-----')\n",
    "print('Precision Score : {}'.format(precision_score(y_test,y_pred)))\n",
    "print('Recall Score : {}'.format(recall_score(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(rec,pre)\n",
    "plt.xlabel('recall')\n",
    "plt.ylabel('precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre,rec,thr=precision_recall_curve(y_test,proba_up)\n",
    "plt.title('with up sampling')\n",
    "plt.plot(thr, pre[:-1], 'b--', label='precision')\n",
    "plt.plot(thr, rec[:-1], 'g--', label = 'recall')\n",
    "plt.xlabel('Threshold')\n",
    "plt.legend(loc='upper left')\n",
    "plt.ylim([0,1])\n",
    "plt.show()\n",
    "print('-----with up sampling-----')\n",
    "print('Precision Score : {:.2}'.format(precision_score(y_test,y_up_pred)))\n",
    "print('Recall Score : {:.2}'.format(recall_score(y_test,y_up_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(rec,pre)\n",
    "plt.xlabel('recall')\n",
    "plt.ylabel('precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre,rec,thr=precision_recall_curve(y_test,proba_down)\n",
    "plt.title('with up sampling')\n",
    "plt.plot(thr, pre[:-1], 'b--', label='precision')\n",
    "plt.plot(thr, rec[:-1], 'g--', label = 'recall')\n",
    "plt.xlabel('Threshold')\n",
    "plt.legend(loc='upper left')\n",
    "plt.ylim([0,1])\n",
    "plt.show()\n",
    "print('-----with down sampling-----')\n",
    "print('Precision Score : {:.2}'.format(precision_score(y_test,y_down_pred)))\n",
    "print('Recall Score : {:.2}'.format(recall_score(y_test,y_down_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(rec,pre)\n",
    "plt.xlabel('recall')\n",
    "plt.ylabel('precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "비슷한 특성을 가지는 feature들을 group화 하여 각 group에서 PCA를 진행한 후에 concat하는 방법을 시도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=final_data.loc[:,final_data.columns!='acrn_yn']\n",
    "y=final_data.loc[:,final_data.columns=='acrn_yn']\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,stratify=y)\n",
    "X_up,y_up=up.fit_resample(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_up=pd.DataFrame(X_up,columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'height' in X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#related to weight #4 features => 1\n",
    "g1=[ 'bmi', 'obesity', 'wt_circ', 'abd_obe']\n",
    "\n",
    "#related to blood #10 features => 7 => 영향 많음\n",
    "g2=[ 'hba1c', 'hba1c_65', 'rbc',  'hct','ob_yn','ferritin']\n",
    "\n",
    "#related to nutrient #4 features => 1\n",
    "g3=['ca', 'glucose', 'glu_100','dm']\n",
    "\n",
    "#related to protein #1 features => 1\n",
    "g4=[ 'hs_crp']\n",
    "\n",
    "#related to life habit #3 features => 1\n",
    "g5=['smoke']\n",
    "\n",
    "#related to hormone #4 features => 1\n",
    "g6=['ldh','tsh', 'insulin', 'homa_ir']\n",
    "\n",
    "#related to cancer #9 features => 7 => 영향 많음\n",
    "g7=['meta_yn', 'mets_yn', 'meta_n', 'obe_meta','ft4','ft3','cea','crc_fhx']\n",
    "\n",
    "#related to blood pressure #3 features => 1\n",
    "g8=['htn', 'sbp', 'dbp']\n",
    "\n",
    "#related to lipid #7 features => 5 => 영향 많음\n",
    "g9=['t_chol',  'tg', 'tg_150', 'hdl', 'ldl']\n",
    "\n",
    "#related to personal stat #3 features => 1\n",
    "g10=['sex', 'age'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas=[X_up[g1],X_up[g2],X_up[g3],X_up[g4],X_up[g5],X_up[g6],X_up[g7],X_up[g8],X_up[g9],X_up[g10]]\n",
    "# datas=[X_up[g1],X_up[g3],X_up[g4],X_up[g5],X_up[g6],X_up[g8],X_up[g10]]\n",
    "y=data['acrn_yn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_up.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "nums=[1,4,1,1,1,1,5,1,3,1]\n",
    "pcas=[]\n",
    "reduced=[]\n",
    "\n",
    "for i in range(10):\n",
    "    pcas.append(PCA(n_components=nums[i]))\n",
    "    reduced.append(pcas[i].fit_transform(datas[i],y_up))\n",
    "    \n",
    "X_concat=np.hstack(reduced)\n",
    "# X_concat_all=np.hstack([X_concat,X_up[g2].values,X_up[g7].values,X_up[g9].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X_concat,y_up,test_size=0.3,stratify=y_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=XGBClassifier()\n",
    "clf.fit(X_train,y_train)\n",
    "pred=clf.predict(X_test)\n",
    "accuracy_score(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#Common Model Helpers\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-205-6e3765bc2954>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0malg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMLA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mpipeline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pca'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpca\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'MLA'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_up_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_up_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m     \u001b[0my_up_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_up_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;31m#set name and parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    265\u001b[0m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m         \u001b[1;31m# Fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 412\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    141\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m                 \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m                 random_state)\n\u001b[0m\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m             \u001b[1;31m# Early termination\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36m_boost\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    470\u001b[0m         \"\"\"\n\u001b[0;32m    471\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithm\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'SAMME.R'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 472\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_boost_real\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miboost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    473\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# elif self.algorithm == \"SAMME\":\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36m_boost_real\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    480\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m         \u001b[0my_predict_proba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    799\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    800\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 801\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m    802\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    803\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    364\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 366\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Machine Learning Algorithm (MLA) Selection and Initialization\n",
    "MLA = [\n",
    "    #Ensemble Methods\n",
    "    ensemble.AdaBoostClassifier(),\n",
    "    ensemble.BaggingClassifier(),\n",
    "    ensemble.ExtraTreesClassifier(),\n",
    "    ensemble.GradientBoostingClassifier(),\n",
    "    ensemble.RandomForestClassifier(),\n",
    "\n",
    "    #Gaussian Processes\n",
    "#     gaussian_process.GaussianProcessClassifier(),\n",
    "    \n",
    "    #GLM\n",
    "    linear_model.LogisticRegressionCV(),\n",
    "    linear_model.PassiveAggressiveClassifier(),\n",
    "    linear_model.RidgeClassifierCV(),\n",
    "    linear_model.SGDClassifier(),\n",
    "    linear_model.Perceptron(),\n",
    "    \n",
    "    #Navies Bayes\n",
    "    naive_bayes.BernoulliNB(),\n",
    "    naive_bayes.GaussianNB(),\n",
    "    \n",
    "    #Nearest Neighbor\n",
    "    neighbors.KNeighborsClassifier(),\n",
    "    \n",
    "    #SVM\n",
    "#     svm.SVC(probability=True),\n",
    "#     svm.NuSVC(probability=True),\n",
    "    svm.LinearSVC(),\n",
    "    \n",
    "    #Trees    \n",
    "    tree.DecisionTreeClassifier(),\n",
    "    tree.ExtraTreeClassifier(),\n",
    "    \n",
    "    #Discriminant Analysis\n",
    "    discriminant_analysis.LinearDiscriminantAnalysis(),\n",
    "    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n",
    "\n",
    "    \n",
    "    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n",
    "    XGBClassifier()    \n",
    "    ]\n",
    "\n",
    "#create table to compare MLA metrics\n",
    "MLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy', 'MLA Test Accuracy', 'MLA Precision', 'MLA Recall']\n",
    "MLA_compare = pd.DataFrame(columns = MLA_columns)\n",
    "\n",
    "row_index = 0\n",
    "for alg in tqdm(MLA):\n",
    "    pipeline=Pipeline([('pca',pca),('MLA',alg)])\n",
    "    pipeline.fit(X_up_train, y_up_train)\n",
    "    y_up_pred = pipeline.predict(X_up_test)\n",
    "    #set name and parameters\n",
    "    MLA_name = alg.__class__.__name__\n",
    "    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n",
    "    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n",
    "    \n",
    "    MLA_compare.loc[row_index, 'MLA Train Accuracy'] = pipeline.score(X_up_train, y_up_train)\n",
    "    MLA_compare.loc[row_index, 'MLA Test Accuracy'] = metrics.accuracy_score(y_up_test, y_up_pred)\n",
    "    MLA_compare.loc[row_index, 'MLA Precision'] = metrics.precision_score(y_up_test, y_up_pred)\n",
    "    MLA_compare.loc[row_index, 'MLA Recall'] = metrics.recall_score(y_up_test, y_up_pred)\n",
    "\n",
    "    row_index+=1\n",
    "\n",
    "MLA_compare.sort_values(by = ['MLA Test Accuracy'], ascending = False, inplace = True)\n",
    "MLA_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "This notebook is created by learning from the following notebooks:\n",
    "- [Titanic: Machine Learning from Disaster ( Minsuk Heo )](https://github.com/minsuk-heo/kaggle-titanic/blob/master/titanic-solution.ipynb)\n",
    "- [Minsuk Heo's Pandas Cheat Sheet](https://github.com/minsuk-heo/pandas/blob/master/Pandas_Cheatsheet.ipynb)\n",
    "- [Python으로 하는 탐색적 자료 분석 (Exploratory Data Analysis)](https://3months.tistory.com/325)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.sample(frac,replace,random_state)\n",
    "# #time series\n",
    "# data.groupby(pd.Grouper(freq='M'))\n",
    "# pd.to_datetime()\n",
    "# pd.to_numeric()\n",
    "# data.plot(kind,figsize)\n",
    "# data.boxplot()\n",
    "# data.value_counts()\n",
    "# bins=pd.cut(data[col],4) #동일한 width 기준으로 4개의 bin으로 grouping 함\n",
    "# bins=pd.qcut(data[col],4) #동일한 양을 기준으로 4개의 bin으로 grouping 함\n",
    "\n",
    "# from sklearn import metrics\n",
    "# metrics.classification_report(y_true,y_pred,labels=[0,1])\n",
    "\n",
    "# from sklearn.metrics import confusion_matrix,recall_score,precision_score,f1_score\n",
    "# confusion_matrix(y_true,y_pred)\n",
    "# precision_score(y_true,y_pred)\n",
    "# f1_score(y_true,y_pred)\n",
    "# recall_score(y_true,y_pred)\n",
    "\n",
    "# from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "# y_pred=cross_val_predict(clf,x_train,y_train,cv,method)\n",
    "\n",
    "# from sklearn.metrics import roc_auc_curve,precision_recall_curve,roc_curve\n",
    "# imbalanced data에 대해서는 PR곡선이 더 나음\n",
    "\n",
    "#for imbalanced dataset\n",
    "#1. sampling method -resampling\n",
    "#                   -upsampling\n",
    "#                   -downsampling\n",
    "# !pip install -U imbalanced-learn\n",
    "# from imblearn.under_sampling import RandomUnderSampler as UnderSampler\n",
    "# from imblearn.over_sampling import RandomOverSampler as OverSampler\n",
    "\n",
    "\n",
    "#2. class weighted / cost-sensitive learning\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# class_weights=compute_class_weight('balanced',np.unique(y),y)\n",
    "\n",
    "#tf.nn.weighted_cross_entropy_with_logits or keras fit(X,y,class_weights)\n",
    "#scikit-learn class_weight parameter for such clf ex) SVM , RF\n",
    "\n",
    "# from xgboost.sklearn import XGBClassifier\n",
    "# from sklearn.grid_search import GridSearchCV\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# feature selection, column importance(pearson, RF, xgb, PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 해야할 일\n",
    "-upsampling / downsampling 해서 pos/neg imbalance 개선하기\n",
    "\n",
    "-column들 min-max scaling => xgboost는 tree기반이기에 scaling은 속도에만 영향을 미치고 성능에는 영향을 안 미침\n",
    "\n",
    "-xgb로 all feature에 대해 해보고 feature importance 구해보기 => feature selection!\n",
    "\n",
    "-sklearn RFE로 feature importance?\n",
    "\n",
    "-sklearn RF로 feature importance?\n",
    "\n",
    "-pca로 dimension reduction한 data에 대해 xgb classify해보기 \n",
    "\n",
    "-GridSearch로 best parameter 찾기\n",
    "\n",
    "-PR value / PR 곡선 / confusion matrix 구해보기\n",
    "\n",
    "-feature generation? (feature들의 pearson 상관계수, PCA, aggregation, 비슷한 feature drop, clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 더 해야 할 일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- xgb hyper parameter gridsearch로 최적화\n",
    "- feature generation으로 접근해서 새로운 feature들(aggregation or conjugation)를 생성하고 RFE나 RF를 통해 important feature selection 해서 xgb 다시 해보기"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "275.719px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
