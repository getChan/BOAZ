- bypass는 이러한 매우 깊은 네트워크의 교육을 용이하게하는 핵심 요소로 간주됩니다.
- resnet이 대표적
  - ResNet은 ImageNet 및 COCO 객체 탐지와 같은 많은 도전적인 이미지 인식, 로컬라이제이션 및 탐지 작업에서 인상적이고 기록적인 성능을 달성했습니다
- 최근에, 1202 층의 ResNet을 성공적으로 훈련시키는 방법으로 stochastic depth가 제안되었습니다
  - stochastic depth는 훈련 도중 무작위로 레이어를 drop하여 deep residual 네트워크의 훈련을 향상시킵니다.
  - 이것은 모든 레이어가 필요하지 않으며 deep (residual) 네트워크에 많은 양의 중복성이 있음을 강조합니다.
- 우리의 논문은 그 관찰에 부분적으로 영감을 받았습니다. pre-activation ResNets는 1000 개 이상의 레이어를 갖춘 최첨단 네트워크의 교육을 용이하게합니다
- ------------------
- 네트워크를 더 깊게 만드는 직각 접근법 (예를 들어, 건너 뛰기 연결의 도움)은 네트워크 너비를 증가시키는 것입니다.
- GoogLeNet [36, 37]은 크기가 다른 필터로 생성 된 피쳐맵을 연결하는 "Inception module"을 사용합니다.
- [38]에서, 넓은 일반화 된 residual 블록을 가진 ResNets의 변형이 제안되었다.
- 사실, 깊이가 충분할 떄 간단히 각 레이어 필터의 수를 증가시키면 성능이 향상된다.
-  FractalNets은 또한 광범위한 네트워크 구조를 사용하여 여러 데이터 세트에서 경쟁력있는 결과를 얻습니다.
-  ----------------
- **극도로 깊거나 넓은 아키텍처에서 표현력을 끌어내는 대신 DenseNets는 feature-reuse를 통해 네트워크의 잠재력을 활용하여 교육하기 쉽고 고도로 매개 변수가 없는 응축 모델을 산출합니다.**
  - 서로 다른 레이어에서 학습 된 피쳐 맵을 연결하면 후속 레이어의 입력 변화가 증가하고 효율성이 향상됩니다.
  - 이것은 DenseNets와 ResNets 사이의 주요 차이점을 구성합니다.
  - 서로 다른 레이어의 피쳐를 연결하는 인셉션 네트워크 [36, 37]와 비교하여 DenseNets은 더 간단하고 효율적입니다.
  - ------------------------------------
- 경쟁력있는 결과를 이끌어 낸 다른 주목할만한 네트워크 아키텍처 혁신이 있습니다. 
  - The Network In Network (NIN)[22] 구조는 컨볼 루션 레이어의 필터에 마이크로 멀티 레이어 퍼셉트론을 포함하여 더 복잡한 기능을 추출합니다
  - DSN (Deeply Supervised Network) [20]에서, 내부 레이어는 이전 분류에 의해 수신 된 그라디언트를 강화할 수있는 보조 분류 자에 의해 직접 감독된다
  - Ladder Networks [27, 25]는 오토인코더에 측면 연결을 도입하여 준지도 학습 과제에 대한 인상적인 정확성을 제공합니다
  - DFN (Deeply-Fused Nets)은 서로 다른 기본 네트워크의 중간 계층을 결합하여 정보 흐름을 개선하기 위해 제안되었습니다
- 재구성 손실을 최소화하는 경로가있는 네트워크의 증가는 이미지 분류 모델을 향상시키는 것으로 나타났다