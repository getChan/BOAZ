> 밑바닥부터 시작하는 딥러닝 **2**
> 베타리딩 정리입니다.
# 2. 자연어와 단어의 분산 표현
## 2.1 자연어 처리란
### 2.1.1 단어의 의미
* 시소러스를 활용한 기법 (이번 장)
* 통계 기반 기법 (이번 장)
* 추론 기반 기법 (word2vec) (다음 장)
## 2.2 시소러스
- 시소러스는 유의어 사전으로, '뜻이 같은 단어(동의어)' 나 뜻이 비슷한 단어(유의어)' 가 한 그룹으로 분류되어 있다.
- 단어들을 의미의 상.하위 관계에 기초해 그래프로 표현한다.
### 2.2.1 WordNet
- 단어 네트워크를 사용해 단어 사이의 유사도를 구한다.
### 2.2.2 시소러스의 문제점
* 시대 변화에 대응하기 어렵다.
* 사람을 쓰는 비용**은** 크다 <- 오탈자
* 단어의 미묘한 차이를 표현할 수 없다.
	-> '통계 기반 기법', 신경망 사용한 '추론 기반 기법'
	-> 단어의 의미를 자동으로 추출한다.
## 2.3 통계 기반 기법
- 말뭉치(corpus) : 대량의 텍스트 데이터
### 2.3.1 파이썬으로 말뭉치 전처리하기
- corpus : 단어 id 목록
- word_to_id : 단어에서 ID로의 딕셔너리
- id_to_word : ID에서 단어로의 딕셔너리
### 2.3.2 단어의 분산 표현
- 색을 RGB 벡터로 표현하듯 단어도 벡터로 표현하자
- 분산 표현 : '단어의 의미'를 정확하게 파악할 수 있는 벡터 표현
### 2.3.3 분포 가설
- **" 단어의 의미는 주변 단어에 의해 형성된다 "**
- 단어 자체에는 의미가 없고, 단어가 사용된 맥락이 의미를 형성한다.
- 맥락 : 주목하는 단어 주변에 놓은 단어
- 윈도우 크기 : 포함시킬 좌우 단어의 개수
### 2.3.4 동시발생 행렬
모든 단어에 대해 동시발생하는 단어의 행렬
### 2.3.5 벡터 간 유사도
- 코사인 유사도 
	- 벡터의 정규화
	- 내적 구하기
### 2.3.6 유사 단어의 랭킹 표시
- 단어가 주어지만 유사도가 높은 단어를 순서대로 출력
## 2.4 통계 기반 기법 개선하기
> 동시발생 횟수는 고빈도 단어의 관련성을 크게 하는 문제가 있다.
### 2.4.1 상호정보량
- PMI(점별 상호정보량) : pointwise Mutual Information
- $$PMI(x,y) = \log_{2}\frac{P(x,y)}{P(x)P(y)}$$
- 높을수록 관련성이 높다.
- 실제 구현은 양의 상호정보량(PPMI) 사용
- $$PPMI(x,y) = max(0, PMI(x,y))$$
- 문제점
  - 말뭉치의 어휘 수가 증가하면서 단어 벡터의 차원 수도 증가
  - 원소 대부분이 0 -> 원소의 '중요도'가 낮다
  - 노이즈에 약하고 견고하지 못함
  - -> 벡터의 차원 감소 기법으로 대처
### 2.4.2 차원 감소
- 벡터의 차원을 줄인다.
- '중요한 정보'는 유지하면서 차원 감소
- 특잇값분해(SVD) : Singular Value Decomposition
- $$ X = USV^T $$
- 임의의 행렬 X를 U, S, V 세 행렬의 곱으로 분해
- U : 직교행렬, 공간의 축(기저)을 형성, **단어 공간**
- S : 대각행렬, '특잇값(해당 축의 중요도)'
### 2.4.4 PTB 데이터셋
## 2.5 정리
- WordNet 등의 시소러스를 이용하면 유의어를 얻거나 단어 사이의 유사도를 측정하는 등 유용한 작업을 할 수 있다.
- 시소러스 기반 기법은 시소러스를 작성하는 데 엄청난 인적 자원이 든다거나 새로운 단어에 대응하기 어렵다는 문제가 있다.
- 현재는 말뭉치를 이용해 단어를 벡터화하는 방식이 주로 쓰인다.
- 최근의 단어 벡터화 기법들은 대부분 '단어의 의미는 주변 단어에 의해 형성된다'는 분포 가설에 기초한다.
- 통계 기반 기법은 말뭉치 안의 각 단어에 대해서 그 단어의 주변 단어의 빈도를 집계한다.(동시발생 행렬)
- 동시발생 행렬을 PPMI 행렬로 변환하고 다시 차원을 감소시킴으로써, 거대한 '희소벡터'를 작은 '밀집벡터'로 변환할 수 있다.
- 단어의 벡터 공간에서는 의미가 가까운 단어는 그 거리도 가까울 것으로 기대된다.

# Chapter 3. word2vec
> 신경망 기반의 '추론 기반 기법' 
## 3.1 추론 기반 기법과 신경망
### 3.1.1 통계 기반 기법의 문제점
- 대규모 말뭉치를 다룰 때 문제가 발생한다.
- 많은 어휘 수를 가지는 거대 행렬에 SVD를 적용하기 힘들다.
- 추론 기반 기법에서는 미니배치로 한번에 소량의 학습 샘플씩 반복해서 학습하며 가중치를 갱신
### 3.1.2 추론 기반 기법 개요
- 모델은 맥락 정보를 입력받아 각 단어의 출현 확률을 출력
### 3.1.3 신경망에서의 단어 처리
- One-hot vector : 단어를 '고정 길이의 벡터'로 변환


# Chapter 4. Word2vec 속도 개선
- 문제점
  - 말뭉치 어휘 수가 많아지면 계산량도 커진다.
- 개선
  - Embedding 계층 도입
  - 네거티브 샘플링 손실 함수 도입
## 4.1 word2vec 개선 (1)
- 병목 현상
  - 입력층의 원핫 표현과 가중치 행렬의 곱 계산
  - 은닉층과 가중치 행렬의 곱 및 softmax 계층의 계산
### 4.1.1 Embedding 계층
- 결과적으로 수행하는 일은 행렬의 특정 행을 추출하는 것 뿐
- 원 핫 표현으로의 변환과 MatMul 계층의 행렬곱 계산은 필요없다.
* 단어의 밀집벡터 표현을 **단어 임베딩** 혹은 단어의 **분산 표현**이라 한다.
### 4.1.2 Embedding 계층 구현
- Embedding 계층의 순전파는 가중치 W의 특정 행을 추출한다.
- 역전파는 기울기를 그대로 흘려준다.
--------------------------------------------------
왜 더하는지 각주로 알려주면 좋을듯... 잘 모르겟서요;;
## word2vec 개선 (2)
- 은닉 층 이후의 처리
- softmax 대신 **네터티브 샘플링** 이용
### 4.2.1 은닉층 이후 계산의 문제점
### 4.2.2 다중 분류에서 이진 분류로
- 네거티브 샘플링 : 다중 분류를 이진 분류로 근사
- "맥락이 you와 goodbye 일 때, 타깃 단어는 say 입니까?"
- 은닉 층과 출력 측<----오탈자
------------------------------------------------
- 은닉 층과 출력 층의 가중치 행렬의 내적
  - 'say'에 해당하는 열(단어 벡터)만을 추출
  - 추출된 벡터와 은닉층 뉴런과의 내적
- 이전까지의 출력층에서는 모든 단어를 대상으로 계산을 수행
- 여기서는 단어 하나에 주목하여 점수만을 계산
### 4.2.3 시그모이드 함수와 교차 엔트로피 오차
- score에 시그모이드 함수 적용해 확률로 변환
- 손실 함수로 '교차 엔트로피 오차' 사용
### 4.2.4 다중 분류에서 이진 분류로 (구현)
### 4.2.5 네거티브 샘플링
- 긍정적인 예(정답))에 대해서만 학습했다.
- 부정적인 예(오답)를 입력하면 어떤 결과가 나올지 확실하지 않다.
- 모든 부정적 예를 대상으로 이진 분류 학습? -> NO! 근사적으로 몇 개만 선택
- 긍정적 예와 샘플링된 부정적 예의 손실을 더한 값을 최종 손실로 정한다.
### 4.2.6 네거티브 샘플링의 샘플링 기법
- 부정적 예의 샘플링
  - 말뭉치의 통계 데이터를 기초로 샘플링
  - 말뭉치에서 자주 등장하는 단어를 많이 추출
  - 출현 횟수를 바탕으로 확률분포를 구해 샘플링 수행
  - 확률분포에 0.75 제곱 -> 출현 확률이 낮은 단어를 버리지 않기 위함
### 4.2.7 네거티브 샘플링 구현
## 4.3 개선판 word2vec 학습
### 4.3.1 CBOW 모델 구현
### 4.3.2 CBOW 모델 학습 코드
### 4.3.3 CBOW 모델 평가
- 단어의 관계, 시제정보, 단복수, 비교급 등의 분산 표현에 인코딩됨
## 4.4 word2vec 남은 주제
### 4.4.1 word2vec을 사용한 애플리케이션의 예
- **전이 학습** : 한 분야에서 배운 지식을 다른 분야에도 적용
  - 텍스트 분류, 문서 클러스터링, 품사 태깅, 감정 분석 등
- 문장(단어의 흐름)도 분산 표현을 사용하여 고정 길이 벡터로 변환
  - bag-of-words
    - 문장의 각 단어를 분산 표현으로 변환하고 그 합을 구한다.
    - 단어의 순서를 고려하지 않는 모델
  - RNN
- 자연어를 벡터로 변환할수 있다 -> 일반적인 머신러닝 기법 적용 가능
### 4.4.2 단어 벡터 평가 방법
- 단어의 분산 표현을 학습하는 시스템과 
- 분류하는 시스템의 학습을 따로 수행할 수 있다.
  - 유사성 평가 : 사람이 직접 단어 사이의 유사한 정보 규정, word2vec의 코사인 유사도 점수를 비교하여 상관성 분석
  - 유추 문제 평가 : 유추 문제를 출제하고, 정답률로 단어 분산 표현의 우수성을 측정
    - 의미(semantics) : 단어의 의미를 유추하는 문제의 정답률
    - 구문(syntax) : 단어의 형태 정보 문제의 정답률
    - 모델에 따라 정확도가 다릅니다
    - 말뭉치가 클수록 결과가 좋습니다
    - 단어 벡터 차원 수는 적당한 크기가 좋습니다.
- 단어의 분산 표현의 우수함은 다루는 문제 상황에 따라 다르다.
## 4.5 정리
- Embedding 계층은 단어의 분산 표현을 담고 있으며, 순전파 시 지정한 단어 ID의 벡터를 추출한다.
- word2vec은 어휘 수의 증가에 비례하여 계산량도 증가하므로, 근사치로 계산하는 빠른 기법을 사용하면 좋다.
- 네거티브 샘플링은 부정적 예를 몇 개 샘플링하는 기법으로, 이를 이용하면 다중 분류를 이진 분류처럼 취급할 수 있다.
- word2vec으로 얻은 단어의 분산 표현에는 단어의 의미가 녹아들어 있으며, 비슷한 맥락에서 사용되는 단어는 단어 벡터 공간에서 가까이 위치한다.
- word2vec의 단어의 분산 표현을 이용하면 유추 문제를 벡터의 덧셈과 뺄셈으로 풀 수 있게 된다.
- word2vec은 전이 학습 측면에서 특히 중요하며, 그 단어의 분산 표현은 다양한 자연어 처리 작업에 이용할 수 있다.

# 5. 순환 신경망(RNN)
## 5.1 확률과 언어 모델
### 5.1.1 word2vec을 확률 관점에서 바라보다
- CBOW모델은 맥락 wt-1과 wt+1로부터 타깃 wt를 추측하는 일을 수행
  - P(wt|wt-1, wt+1)
- 맥락을 왼쪽 윈도우로 한정하면
  - P(wt|wt-2, wt-1)
- 손실 함수
  - L = -logP(wt|wt-2, wt-1)
### 5.1.2 언어 모델
- 단어 나열에 확률을 부여함
- 특정 단어 시퀀스에 대해 시퀀스가 일어날 확률
- 

### 5.5.2 언어 모델의 평가

- **퍼플렉서티**를 이용하여 언어 모델의 예측 성능을 평가
  - 확률의 역수
  - 다음에 출현할 수 있는 단어의 후보 수



## 5.3 RNN 구현

## 5.4 시계열 데이터 처리 계층 구현

## 5.5 RNNLM 학습과 평가

## 5.6 정리

- RNN은 순환하는 경로가 있고, 이를 통해 내부에 `은닉상태` 를 기억할 수 있다.
- RNN의 순환 경로를 펼침으로써 다수의 RNN 계층이 연결된 신경망으로 해석할 수 있으며, 보통의 오차역전파법으로 학습할 수 있다(=BPTT)
- 긴 시계열 데이터를 학습할 때는 데이터를 적당한 길이씩 모으고(블록), 블록 단위로 BPTT에 의한 학습을 수행한다(=Truncated BPTT)
- Truncated BPTT에서는 역전파의 연결만 끊는다
- Truncated BPTT에서는 순전파의 연결을 유지하기 위해 데이터를 '순차적'으로 입력해야 한다
- 언어 모델은 단어 시퀀스를 확률로 해석한다
- RNN 계층을 이용한 조건부 언어 모델은(이론적으로는) 그때까지 등장한 모든 단어의 정보를 기억할 수 있다.

# 6. 게이트가 추가된 RNN

- LSTM. GRU와 같이 `장기기억`을 가능하게 하는 매커니즘

## 6.1 RNN의 문제점

- RNN은 시계열 데이터의 장기 의존 관계 학습 어려움
  - BPTT에서 기울기 소실 혹은 기울기 폭발이 일어남

### 6.1.1 RNN 복습

### 6.1.2 기울기 소실 또는 기울기 폭발

### 6.1.3 기울기 소실과 기울기 폭발의 원인

- 역전파로 전해지는 기울기는 순서대로
  - `tahn `: 미분값은 0 ~ 1.0 이며, 0에서 멀어질수록 작아진다.
    - 기울기가 `tanh`노드를 지날 때마다 계속 작아진다
    - ReLU는 기울기 소실을 줄여줌
  - `+` :  그대로 흘려보냄
  - `MatMul` : $ dhW_h^T$ 라는 행렬 곱으로 기울기 계산
    - 같은 $W_h$ 계산을 시계열 데이터의 시간 크기(T)만큼 반복
    - **기울기 dh는 시간 크기에 비례해 지수적으로 증가/감소한다** 
    - Wh가 1보다 크면 기울기 폭발, Wh가 1보다 작으면 기울기 소실
      - 행렬의 경우 `특잇값`

### 6.1.4 기울기 폭발 대책

`기울기 클리핑`
$$
if ||\hat g|| \ge threshold:
\hat g = \frac{threshold}{||\hat g||}\hat g
$$

- $\hat g $ 는 모든 매개변수의 기울기를 모은 것

## 6.2 기울기 소실과 LSTM

### 6.2.1 LSTM의 인터페이스

- LSTM 계층의 인터페이스(입출력)에는 **c**라는 경로가 있다.
  - **c**는 **기억 셀**로 LSTM 전용의 기억 매커니즘이다.
  - 기억 셀은 데이터를 LSTM 계층 내에서만 주고받는다.
  - 은닉 상태 **h**는 다른 계층으로(위쪽으로) 출력된다.

### 6.2.2 LSTM 계층 조립하기

- ![1563280725628](C:\Users\skarn\AppData\Roaming\Typora\typora-user-images\1563280725628.png)
- $c_i$는 3개의 입력($c_{i-1}, h_{t-1}, x_i$)으로부터 '어떤 계산'을 수행하여 구한다.
  - 핵심은 갱신된 $c_i$를 이용하여 다음 은닉 상태 $h_i$를 구한다는 것.
  - $h_i = tanh(c_i)$
- LSTM은 **게이트**를 어느 정도 열지 데이터로부터 학습한다.
  - 게이트는 열고 닫고 데이터의 흐름 조절

### 6.2.3 output 게이트

- tanh$(c_i)$에 게이트를 적용해본다.
- 게이트를 얼마나 열지 o 는 입력$x_i$와 이전 상태 $h_{t-1}$로부터 구한다.

$$
o = \sigma(x_tW_x^{(o)}+h_{t-1}W_h^{(o)}+b^{(o)})
$$

- ![1563281977909](C:\Users\skarn\AppData\Roaming\Typora\typora-user-images\1563281977909.png)
  $$
  h_t = o \odot tanh(c_t)
  $$
  - $\odot$은 아다마르 곱으로, 원소별 곱을 수행한다.

> (주로) 게이트에서는 시그모이드 함수가, '정보' 데이터에는 tanh 함수가 사용된다.

### 6.2.4 forget 게이트

- $c_i$의 기억 중 불필요한 기억을 잊게 해주는 게이트를 추가

- ![1563282165357](C:\Users\skarn\AppData\Roaming\Typora\typora-user-images\1563282165357.png)

- $$
  f = \sigma(x_tW_x^{(f)}+h_{t-1}W_h^{(f)}+b^{(f)})
  $$

$$
c_t = f\odot c_{t-1}
$$

### 6.2.5 새로운 기억 셀

- 새로 기억해야 할 정보를 기억 셀에 추가하자

- ![1563282952349](C:\Users\skarn\AppData\Roaming\Typora\typora-user-images\1563282952349.png)

- $$
  g = tanh(x_tW_x^{(g)}+h_{t-1}W_h^{(g)}+b^{(g)})
  $$

### 6.2.6 input 게이트

- **g**에 추가하는 게이트

  - **g**의 각 원소가 새로 추가되는 정보로써의 가치가 얼마나 큰지 판단
  - 동시에 가중된 정보가 새로 추가된다.

  ![1563283483515](C:\Users\skarn\AppData\Roaming\Typora\typora-user-images\1563283483515.png)

- $$
  i = \sigma(x_tW_x^{(i)}+h_{t-1}W_h^{(i)}+b^{(i)})
  $$

### 6.2.7 LSTM의 기울기 흐름

- 어떻게 기울기 소실 문제를 없앨까? 기억 셀 **c** 의 역전파를 살펴보자
  - `+` 노드는 상류에서 전해지는 기울기를 그대로 흘려보낸다
  - `X` 노드는 '행렬 곱' 이 아닌 '원소별 곱'을 계산한다.
    - 또한 매 t 마다 다른 게이트 값을 이용해 원소별 곱을 계산한다.
      - RNN은 똑같은 가중치 행렬로 '행렬 곱'을 반복
    - forget 게이트가 '잊어야 한다' 하면 기울기가 작아지고
    - '기억해야 한다' 하면 기울기가 작아지지 않고 역전파

## 6.3 LSTM 구현

### 6.3.1 Time LSTM 구현

- T 개분의 시계열 데이터를 한꺼번에 처리하는 계층

## 6.4 LSTM을 사용한 언어 모델

## 6.5 RNNLM 추가 개선

### 6.5.1 LSTM 계층 다층화

- LSTM 계층을 여러겹 쌒아 모델 정확도를 높일 수 있다.

### 6.5.2 드롭아웃에 의한 과적합 억제

- RNN은 일반적인 피드포워드 신경망보다 과적합이 쉽다.
  - 가중치가 커지면 페널티를 부과하는 **정규화**
  - 뉴런을 무시하는 **드롭아웃**
- LSTM 계층의 시계열 방향으로 드롭아웃 계층을 삽입? <- 좋지 않다
  - 학습 시 시간이 흐름에 따라서 정보가 사라진다.
  - 흐르는 시간에 비례해 드롭아웃에 의한 노이즈 축적
- 드롭아웃 계층을 깊이 방향으로 적용 <- 조오아
- **변형 드롭아웃** 은 시간 방향으로도 적용하는데 성공했다.
  - 같은 계층의 드롭아웃 간 같은 마스크를 공유한다.

### 6.5.3 가중치 공유

- Embedding 계층의 가중치화 Affine 계층의 가중치를 공유한다.
  - 매개변수의 수가 줄고 정확도도 향상된다.
  - 차원 형상이 서로 전치이다.
    - Embedding 계층의 가중치 전치하여 설정하면 됨!

### 6.5.4 개선된 RNNLM 구현

### 6.5.5 첨단 연구로

## 6.6 정리

- 단순한 RNN의 학습에서는 기울기 소실과 기울기 폭발이 문제가 된다
- 기울기 폭발에는 기울기 클리핑, 기울기 소실에는 게이트가 추가된 RNN(LSTM과 GRU등)이 효과적이다.
- LSTM에는 input게이트, forget게이트, output게이트 3개의 게이트가 있다
- 게이트에는 전용 가중치가 있으며, 시그모이드 함수를 사용하여 0.0~1.0 사이의 실수를 출력한다.
- 언어 모델 개선에는 LSTM 계층 다층화, 드롭아웃, 가중치 공유 등의 기법이 효과적이다.
- RNN의 정규화는 중요한 주제이며, 드롭아웃 기반의 다양한 기법이 제안되고 있다.



# 7. RNN을 사용한 문장 생성