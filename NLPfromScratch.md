> 밑바닥부터 시작하는 딥러닝 **2**
# 2. 자연어와 단어의 분산 표현
## 2.1 자연어 처리란
### 2.1.1 단어의 의미
* 시소러스를 활용한 기법 (이번 장)
* 통계 기반 기법 (이번 장)
* 추론 기반 기법 (word2vec) (다음 장)
## 2.2 시소러스
- 시소러스는 유의어 사전으로, '뜻이 같은 단어(동의어)' 나 뜻이 비슷한 단어(유의어)' 가 한 그룹으로 분류되어 있다.
- 단어들을 의미의 상.하위 관계에 기초해 그래프로 표현한다.
### 2.2.1 WordNet
- 단어 네트워크를 사용해 단어 사이의 유사도를 구한다.
### 2.2.2 시소러스의 문제점
* 시대 변화에 대응하기 어렵다.
* 사람을 쓰는 비용**은** 크다 <- 오탈자
* 단어의 미묘한 차이를 표현할 수 없다.
	-> '통계 기반 기법', 신경망 사용한 '추론 기반 기법'
	-> 단어의 의미를 자동으로 추출한다.
## 2.3 통계 기반 기법
- 말뭉치(corpus) : 대량의 텍스트 데이터
### 2.3.1 파이썬으로 말뭉치 전처리하기
- corpus : 단어 id 목록
- word_to_id : 단어에서 ID로의 딕셔너리
- id_to_word : ID에서 단어로의 딕셔너리
### 2.3.2 단어의 분산 표현
- 색을 RGB 벡터로 표현하듯 단어도 벡터로 표현하자
- 분산 표현 : '단어의 의미'를 정확하게 파악할 수 있는 벡터 표현
### 2.3.3 분포 가설
- **" 단어의 의미는 주변 단어에 의해 형성된다 "**
- 단어 자체에는 의미가 없고, 단어가 사용된 맥락이 의미를 형성한다.
- 맥락 : 주목하는 단어 주변에 놓은 단어
- 윈도우 크기 : 포함시킬 좌우 단어의 개수
### 2.3.4 동시발생 행렬
모든 단어에 대해 동시발생하는 단어의 행렬
### 2.3.5 벡터 간 유사도
- 코사인 유사도 
	- 벡터의 정규화
	- 내적 구하기
### 2.3.6 유사 단어의 랭킹 표시
- 단어가 주어지만 유사도가 높은 단어를 순서대로 출력
## 2.4 통계 기반 기법 개선하기
> 동시발생 횟수는 고빈도 단어의 관련성을 크게 하는 문제가 있다.
### 2.4.1 상호정보량
- PMI(점별 상호정보량) : pointwise Mutual Information
- $$PMI(x,y) = \log_{2}\frac{P(x,y)}{P(x)P(y)}$$
- 높을수록 관련성이 높다.
- 실제 구현은 양의 상호정보량(PPMI) 사용
- $$PPMI(x,y) = max(0, PMI(x,y))$$
- 문제점
  - 말뭉치의 어휘 수가 증가하면서 단어 벡터의 차원 수도 증가
  - 원소 대부분이 0 -> 원소의 '중요도'가 낮다
  - 노이즈에 약하고 견고하지 못함
  - -> 벡터의 차원 감소 기법으로 대처
### 2.4.2 차원 감소
- 벡터의 차원을 줄인다.
- '중요한 정보'는 유지하면서 차원 감소
- 특잇값분해(SVD) : Singular Value Decomposition
- $$ X = USV^T $$
- 임의의 행렬 X를 U, S, V 세 행렬의 곱으로 분해
- U : 직교행렬, 공간의 축(기저)을 형성, **단어 공간**
- S : 대각행렬, '특잇값(해당 축의 중요도)'
### 2.4.4 PTB 데이터셋
## 2.5 정리
- WordNet 등의 시소러스를 이용하면 유의어를 얻거나 단어 사이의 유사도를 측정하는 등 유용한 작업을 할 수 있다.
- 시소러스 기반 기법은 시소러스를 작성하는 데 엄청난 인적 자원이 든다거나 새로운 단어에 대응하기 어렵다는 문제가 있다.
- 현재는 말뭉치를 이용해 단어를 벡터화하는 방식이 주로 쓰인다.
- 최근의 단어 벡터화 기법들은 대부분 '단어의 의미는 주변 단어에 의해 형성된다'는 분포 가설에 기초한다.
- 통계 기반 기법은 말뭉치 안의 각 단어에 대해서 그 단어의 주변 단어의 빈도를 집계한다.(동시발생 행렬)
- 동시발생 행렬을 PPMI 행렬로 변환하고 다시 차원을 감소시킴으로써, 거대한 '희소벡터'를 작은 '밀집벡터'로 변환할 수 있다.
- 단어의 벡터 공간에서는 의미가 가까운 단어는 그 거리도 가까울 것으로 기대된다.

# Chapter 3. word2vec
> 신경망 기반의 '추론 기반 기법' 
## 3.1 추론 기반 기법과 신경망
### 3.1.1 통계 기반 기법의 문제점
- 대규모 말뭉치를 다룰 때 문제가 발생한다.
- 많은 어휘 수를 가지는 거대 행렬에 SVD를 적용하기 힘들다.
- 추론 기반 기법에서는 미니배치로 한번에 소량의 학습 샘플씩 반복해서 학습하며 가중치를 갱신
### 3.1.2 추론 기반 기법 개요
- 모델은 맥락 정보를 입력받아 각 단어의 출현 확률을 출력
### 3.1.3 신경망에서의 단어 처리
- One-hot vector : 단어를 '고정 길이의 벡터'로 변환
- 단어를 벡터로 나타낼 수 있고, 신경망 '계층'들은 벡터를 처리할 수 있다.
  - 단어를 신경망으로 처리할수 있다.
  - ![1564561739402](C:\Users\skarn\AppData\Roaming\Typora\typora-user-images\1564561739402.png)
- 결과적으로 행렬곱을 통해 가중치의 행벡터 하나를 뽑아낸다.
  - ![1564561804763](C:\Users\skarn\AppData\Roaming\Typora\typora-user-images\1564561804763.png)

## 3.2 단순한 word2vec

### 3.2.1 CBOW 모델의 추론 처리

CBOW 모델은 맥락으로부터 **타깃**을 추측하는 신경망

![1564562158274](C:\Users\skarn\AppData\Roaming\Typora\typora-user-images\1564562158274.png)

- 은닉층 뉴런은 입력층의 fully-connected에 의해 계산된다. 입력층이 여러개면 평균낸다.
  - $\frac{1}{2}(h_1+h_2)$
- 출력층 뉴런은 각 단어에 대응하는 '점수'를 뜻한다.
  - 소프트맥스 적용해서 '확률' 얻을 수 있음
- 여기서 $W_{in}$이 단어의 **분산 표현**이다.
  - 단어의 의미가 잘 녹아들어 있다.

> 은닉층의 뉴런 수가 입력층보다 적어야 '간결하게' 정보를 담은 밀집벡터를 얻을 수 있다. 
>
> 이 은닉층의 정보가 '인코딩'

### 3.2.2 CBOW 모델의 학습

- 소프트맥스로 점수를 확률로 변환하고
- 교차 엔트로피 오차로 손실함수 설정

### 3.2.3 word2vec의 가중치와 분산 표현

- 입력 측의 가중치와 출력 층 가중치 모두 분산 표현을 가지고 있다.
- word2vec에서는 '입력 측의 가중치만 이용한다'.
  - glove에서는 둘 다 이용한다.

## 3.3 학습 데이터 준비

### 3.3.1 맥락과 타깃

![1564576629868](C:\Users\skarn\AppData\Roaming\Typora\typora-user-images\1564576629868.png)

![1564576799483](C:\Users\skarn\AppData\Roaming\Typora\typora-user-images\1564576799483.png)

## 3.4 CBOW 모델 구현

## 3.5 word2vec 보충

### 3.5.1 CBOW 모델과 확률

$$
P(w_t | w_{t-1}, w_{t+1})
$$

교차 엔트로피 오차 $L = -\sum_kt_k\log{y_k}$이다. 

- $t_k$는 정답 레이블이며 원핫 벡터로 표현되므로 $w_k$만 1이고 나머지는 0이다

$$
L = -\log P(w_t | w_{t-1}, w_{t+1})
$$

- 따라서 loss function은 확률에 log취한 다음 마이너스 붙이면 된다(**negative log likelihood**)
- 말뭉치 전체로 확장하면

$$
L = -\frac{1}{T}\sum_{t=1}^T\log P(w_t | w_{t-1}, w_{t+1})
$$

### 3.5.2 skip-gram 모델

$$
L = -\frac{1}{T}\sum_{t=1}^T(\log P(w_{t-1} | w_t) +\log P(w_{t+1} | w_t))
$$

### 3.5.3 통계 기반 vs. 추론 기반

| 통계 기반                         | 추론 기반                    |
| :-------------------------------- | :--------------------------- |
| 말뭉치 전체 통계 1회 학습         | 미치배치 학습                |
| 어휘 추가시 일련의 작업 다시 수행 | 매개변수만 다시 학습         |
| 주로 단어의 유사성                | 복잡한 단어 사이의 패턴 파악 |

- 우열을 가릴 수 없다. 
- GloVe : 통계 정보를 손실 함수에 도입해 미니배치 학습

## 3.6 정리

- 추론 기반 기법은 추측하는 것이 목적이며, 그 부산물로 단어의 분산 표현을 얻을 수 있다
- word2vec은 추론 기반 기법이며, 단순한 2층 신경망이다
- wrod2vec은 skip-gram 모델과 CBOW 모델을 제공한다
- CBOW 모델은 여러 단어(맥락)로부터 하나의 단어(타깃)을 추측한다
- Skip-gram은 하나의 단어(타깃)로부터 다수의 단어(맥락)를 추측한다
- word2vec은 가중치를 다시 학습할 수 있으므로, 단어의 분산 표현 갱신이나 새로운 단어 추가를 효율적으로 수행할 수 있다.

# Chapter 4. Word2vec 속도 개선

- 문제점
  - 말뭉치 어휘 수가 많아지면 계산량도 커진다.
- 개선
  - Embedding 계층 도입
  - 네거티브 샘플링 손실 함수 도입
## 4.1 word2vec 개선 (1)
- 병목 현상
  - 입력층의 원핫 표현과 가중치 행렬의 곱 계산
  - 은닉층과 가중치 행렬의 곱 및 softmax 계층의 계산
- 상당한 메모리를 차지하며, 계산 자원을 많이 사용한다.
### 4.1.1 Embedding 계층
- 결과적으로 수행하는 일은 행렬의 특정 행을 추출하는 것 뿐

![1564580042357](C:\Users\skarn\AppData\Roaming\Typora\typora-user-images\1564580042357.png)

- 원 핫 표현으로의 변환과 MatMul 계층의 행렬곱 계산은 필요없다.
* 단어의 밀집벡터 표현을 **단어 임베딩** 혹은 단어의 **분산 표현**이라 한다.
### 4.1.2 Embedding 계층 구현
- Embedding 계층의 순전파는 가중치 W의 특정 행을 추출한다.
- 역전파는 기울기를 그대로 흘려준다.

![1564580281624](C:\Users\skarn\AppData\Roaming\Typora\typora-user-images\1564580281624.png)

왜 더하는지 각주로 알려주면 좋을듯... 잘 모르겟서요;;
## word2vec 개선 (2)
- 은닉 층 이후의 처리
- softmax 대신 **네거티브 샘플링** 이용
### 4.2.1 은닉층 이후 계산의 문제점
### 4.2.2 다중 분류에서 이진 분류로
- 네거티브 샘플링 : 다중 분류를 이진 분류로 근사
- "맥락이 you와 goodbye 일 때, 타깃 단어는 say 입니까?"

![1564580802684](C:\Users\skarn\AppData\Roaming\Typora\typora-user-images\1564580802684.png)

- 은닉 층과 출력 층의 가중치 행렬의 내적
  - 'say'에 해당하는 열(단어 벡터)만을 추출
  - 추출된 벡터와 은닉층 뉴런과의 내적
  
  ![1564580930246](C:\Users\skarn\AppData\Roaming\Typora\typora-user-images\1564580930246.png)
- 이전까지의 출력층에서는 모든 단어를 대상으로 계산을 수행
- 여기서는 단어 하나에 주목하여 점수만을 계산
### 4.2.3 시그모이드 함수와 교차 엔트로피 오차
- score에 시그모이드 함수 적용해 확률로 변환
- 손실 함수로 '교차 엔트로피 오차' 사용
### 4.2.4 다중 분류에서 이진 분류로 (구현)
### 4.2.5 네거티브 샘플링
- 긍정적인 예(정답))에 대해서만 학습했다.
- 부정적인 예(오답)를 입력하면 어떤 결과가 나올지 확실하지 않다.
- 모든 부정적 예를 대상으로 이진 분류 학습? -> NO! 근사적으로 몇 개만 선택
- 긍정적 예와 샘플링된 부정적 예의 손실을 더한 값을 최종 손실로 정한다.
### 4.2.6 네거티브 샘플링의 샘플링 기법
- 부정적 예의 샘플링
  - 말뭉치의 통계 데이터를 기초로 샘플링 **통계적 기법**
  - 말뭉치에서 자주 등장하는 단어를 많이 추출
  - 출현 횟수를 바탕으로 확률분포를 구해 샘플링 수행
  - 확률분포에 0.75 제곱 -> 출현 확률이 낮은 단어를 버리지 않기 위함
### 4.2.7 네거티브 샘플링 구현
## 4.3 개선판 word2vec 학습
### 4.3.1 CBOW 모델 구현
### 4.3.2 CBOW 모델 학습 코드
### 4.3.3 CBOW 모델 평가
- 단어의 관계, 시제정보, 단복수, 비교급 등의 분산 표현에 인코딩됨
## 4.4 word2vec 남은 주제
### 4.4.1 word2vec을 사용한 애플리케이션의 예
- **전이 학습** : 한 분야에서 배운 지식을 다른 분야에도 적용
  - 텍스트 분류, 문서 클러스터링, 품사 태깅, 감정 분석 등
- 문장(단어의 흐름)도 분산 표현을 사용하여 고정 길이 벡터로 변환
  - bag-of-words
    - 문장의 각 단어를 분산 표현으로 변환하고 그 합을 구한다.
    - 단어의 순서를 고려하지 않는 모델
  - RNN
- 자연어를 벡터로 변환할수 있다 -> 일반적인 머신러닝 기법 적용 가능
### 4.4.2 단어 벡터 평가 방법
- 단어의 분산 표현을 학습하는 시스템과 
- 분류하는 시스템의 학습을 따로 수행할 수 있다.
  - 유사성 평가 : 사람이 직접 단어 사이의 유사한 정보 규정, word2vec의 코사인 유사도 점수를 비교하여 상관성 분석
  - 유추 문제 평가 : 유추 문제를 출제하고, 정답률로 단어 분산 표현의 우수성을 측정
    - 의미(semantics) : 단어의 의미를 유추하는 문제의 정답률
    - 구문(syntax) : 단어의 형태 정보 문제의 정답률
    - 모델에 따라 정확도가 다릅니다
    - 말뭉치가 클수록 결과가 좋습니다
    - 단어 벡터 차원 수는 적당한 크기가 좋습니다.
- 단어의 분산 표현의 우수함은 다루는 문제 상황에 따라 다르다.
## 4.5 정리
- Embedding 계층은 단어의 분산 표현을 담고 있으며, 순전파 시 지정한 단어 ID의 벡터를 추출한다.
- word2vec은 어휘 수의 증가에 비례하여 계산량도 증가하므로, 근사치로 계산하는 빠른 기법을 사용하면 좋다.
- 네거티브 샘플링은 부정적 예를 몇 개 샘플링하는 기법으로, 이를 이용하면 다중 분류를 이진 분류처럼 취급할 수 있다.
- word2vec으로 얻은 단어의 분산 표현에는 단어의 의미가 녹아들어 있으며, 비슷한 맥락에서 사용되는 단어는 단어 벡터 공간에서 가까이 위치한다.
- word2vec의 단어의 분산 표현을 이용하면 유추 문제를 벡터의 덧셈과 뺄셈으로 풀 수 있게 된다.
- word2vec은 전이 학습 측면에서 특히 중요하며, 그 단어의 분산 표현은 다양한 자연어 처리 작업에 이용할 수 있다.

# 5. 순환 신경망(RNN)
## 5.1 확률과 언어 모델
### 5.1.1 word2vec을 확률 관점에서 바라보다
- CBOW모델은 맥락 wt-1과 wt+1로부터 타깃 wt를 추측하는 일을 수행
  - P(wt|wt-1, wt+1)
- 맥락을 왼쪽 윈도우로 한정하면
  - P(wt|wt-2, wt-1)
- 손실 함수
  - L = -logP(wt|wt-2, wt-1)
### 5.1.2 언어 모델
- 단어 나열에 확률을 부여함
- 특정 단어 시퀀스에 대해 시퀀스가 일어날 확률

### 5.1.3 CBOW 모델을 언어 모델로?

- 맥락으로 2개의 단어를 이용하자
  - 맥락이 특정 길이로 '고정'된다. <- 문제 발생
- CBOW 모델은 맥락 안의 단어 순서가 무시된다.
  - 맥락의 단어 벡터를 은닉층에 연결하면 된다?
    - 맥락의 크기게 비례해 가중치 매개변수가 늘어난다....

## 5.2 RNN이란

### 5.2.1 순환하는 신경망

### 5.2.2 순환 구조 펼치기

### 5.2.3 BPTT

> 시간 방향으로 펼친 신경망의 오차역전파법

시간 크기가 커지면 메모리 사용량이 증가하고 역전파 기울기가 불안정해진다.

### 5.2.4 Truncated BPTT

### 5.2.5 Trunctaed BPTT의 미니배치 학습



## 5.3 RNN 구현

## 5.4 시계열 데이터 처리 계층 구현

## 5.5 RNNLM 학습과 평가

### 5.5.2 언어 모델의 평가

- **퍼플렉서티**를 이용하여 언어 모델의 예측 성능을 평가
  - 확률의 역수
  - 다음에 출현할 수 있는 단어의 후보 수

## 5.6 정리

- RNN은 순환하는 경로가 있고, 이를 통해 내부에 `은닉상태` 를 기억할 수 있다.
- RNN의 순환 경로를 펼침으로써 다수의 RNN 계층이 연결된 신경망으로 해석할 수 있으며, 보통의 오차역전파법으로 학습할 수 있다(=BPTT)
- 긴 시계열 데이터를 학습할 때는 데이터를 적당한 길이씩 모으고(블록), 블록 단위로 BPTT에 의한 학습을 수행한다(=Truncated BPTT)
- Truncated BPTT에서는 역전파의 연결만 끊는다
- Truncated BPTT에서는 순전파의 연결을 유지하기 위해 데이터를 '순차적'으로 입력해야 한다
- 언어 모델은 단어 시퀀스를 확률로 해석한다
- RNN 계층을 이용한 조건부 언어 모델은(이론적으로는) 그때까지 등장한 모든 단어의 정보를 기억할 수 있다.

# 6. 게이트가 추가된 RNN

- LSTM. GRU와 같이 `장기기억`을 가능하게 하는 매커니즘

## 6.1 RNN의 문제점

- RNN은 시계열 데이터의 장기 의존 관계 학습 어려움
  - BPTT에서 기울기 소실 혹은 기울기 폭발이 일어남

### 6.1.1 RNN 복습

### 6.1.2 기울기 소실 또는 기울기 폭발

### 6.1.3 기울기 소실과 기울기 폭발의 원인

- 역전파로 전해지는 기울기는 순서대로
  - `tahn `: 미분값은 0 ~ 1.0 이며, 0에서 멀어질수록 작아진다.
    - 기울기가 `tanh`노드를 지날 때마다 계속 작아진다
    - ReLU는 기울기 소실을 줄여줌
  - `+` :  그대로 흘려보냄
  - `MatMul` : $ dhW_h^T$ 라는 행렬 곱으로 기울기 계산
    - 같은 $W_h$ 계산을 시계열 데이터의 시간 크기(T)만큼 반복
    - **기울기 dh는 시간 크기에 비례해 지수적으로 증가/감소한다** 
    - Wh가 1보다 크면 기울기 폭발, Wh가 1보다 작으면 기울기 소실
      - 행렬의 경우 `특잇값`

### 6.1.4 기울기 폭발 대책

`기울기 클리핑`
$$
if ||\hat g|| \ge threshold:
\hat g = \frac{threshold}{||\hat g||}\hat g
$$

- $\hat g $ 는 모든 매개변수의 기울기를 모은 것

## 6.2 기울기 소실과 LSTM

### 6.2.1 LSTM의 인터페이스

- LSTM 계층의 인터페이스(입출력)에는 **c**라는 경로가 있다.
  - **c**는 **기억 셀**로 LSTM 전용의 기억 매커니즘이다.
  - 기억 셀은 데이터를 LSTM 계층 내에서만 주고받는다.
  - 은닉 상태 **h**는 다른 계층으로(위쪽으로) 출력된다.

### 6.2.2 LSTM 계층 조립하기

- ![1563280725628](C:\Users\skarn\AppData\Roaming\Typora\typora-user-images\1563280725628.png)
- $c_i$는 3개의 입력($c_{i-1}, h_{t-1}, x_i$)으로부터 '어떤 계산'을 수행하여 구한다.
  - 핵심은 갱신된 $c_i$를 이용하여 다음 은닉 상태 $h_i$를 구한다는 것.
  - $h_i = tanh(c_i)$
- LSTM은 **게이트**를 어느 정도 열지 데이터로부터 학습한다.
  - 게이트는 열고 닫고 데이터의 흐름 조절

### 6.2.3 output 게이트

- tanh$(c_i)$에 게이트를 적용해본다.
- 게이트를 얼마나 열지 o 는 입력$x_i$와 이전 상태 $h_{t-1}$로부터 구한다.

$$
o = \sigma(x_tW_x^{(o)}+h_{t-1}W_h^{(o)}+b^{(o)})
$$

- ![1563281977909](C:\Users\skarn\AppData\Roaming\Typora\typora-user-images\1563281977909.png)
  $$
  h_t = o \odot tanh(c_t)
  $$
  - $\odot$은 아다마르 곱으로, 원소별 곱을 수행한다.

> (주로) 게이트에서는 시그모이드 함수가, '정보' 데이터에는 tanh 함수가 사용된다.

### 6.2.4 forget 게이트

- $c_i$의 기억 중 불필요한 기억을 잊게 해주는 게이트를 추가

- ![1563282165357](C:\Users\skarn\AppData\Roaming\Typora\typora-user-images\1563282165357.png)

- $$
  f = \sigma(x_tW_x^{(f)}+h_{t-1}W_h^{(f)}+b^{(f)})
  $$

$$
c_t = f\odot c_{t-1}
$$

### 6.2.5 새로운 기억 셀

- 새로 기억해야 할 정보를 기억 셀에 추가하자

- ![1563282952349](C:\Users\skarn\AppData\Roaming\Typora\typora-user-images\1563282952349.png)

- $$
  g = tanh(x_tW_x^{(g)}+h_{t-1}W_h^{(g)}+b^{(g)})
  $$

### 6.2.6 input 게이트

- **g**에 추가하는 게이트

  - **g**의 각 원소가 새로 추가되는 정보로써의 가치가 얼마나 큰지 판단
  - 동시에 가중된 정보가 새로 추가된다.

  ![1563283483515](C:\Users\skarn\AppData\Roaming\Typora\typora-user-images\1563283483515.png)

- $$
  i = \sigma(x_tW_x^{(i)}+h_{t-1}W_h^{(i)}+b^{(i)})
  $$

### 6.2.7 LSTM의 기울기 흐름

- 어떻게 기울기 소실 문제를 없앨까? 기억 셀 **c** 의 역전파를 살펴보자
  - `+` 노드는 상류에서 전해지는 기울기를 그대로 흘려보낸다
  - `X` 노드는 '행렬 곱' 이 아닌 '원소별 곱'을 계산한다.
    - 또한 매 t 마다 다른 게이트 값을 이용해 원소별 곱을 계산한다.
      - RNN은 똑같은 가중치 행렬로 '행렬 곱'을 반복
    - forget 게이트가 '잊어야 한다' 하면 기울기가 작아지고
    - '기억해야 한다' 하면 기울기가 작아지지 않고 역전파

## 6.3 LSTM 구현

### 6.3.1 Time LSTM 구현

- T 개분의 시계열 데이터를 한꺼번에 처리하는 계층

## 6.4 LSTM을 사용한 언어 모델

## 6.5 RNNLM 추가 개선

### 6.5.1 LSTM 계층 다층화

- LSTM 계층을 여러겹 쌒아 모델 정확도를 높일 수 있다.

### 6.5.2 드롭아웃에 의한 과적합 억제

- RNN은 일반적인 피드포워드 신경망보다 과적합이 쉽다.
  - 가중치가 커지면 페널티를 부과하는 **정규화**
  - 뉴런을 무시하는 **드롭아웃**
- LSTM 계층의 시계열 방향으로 드롭아웃 계층을 삽입? <- 좋지 않다
  - 학습 시 시간이 흐름에 따라서 정보가 사라진다.
  - 흐르는 시간에 비례해 드롭아웃에 의한 노이즈 축적
- 드롭아웃 계층을 깊이 방향으로 적용 <- 조오아
- **변형 드롭아웃** 은 시간 방향으로도 적용하는데 성공했다.
  - 같은 계층의 드롭아웃 간 같은 마스크를 공유한다.

### 6.5.3 가중치 공유

- Embedding 계층의 가중치화 Affine 계층의 가중치를 공유한다.
  - 매개변수의 수가 줄고 정확도도 향상된다.
  - 차원 형상이 서로 전치이다.
    - Embedding 계층의 가중치 전치하여 설정하면 됨!

### 6.5.4 개선된 RNNLM 구현

### 6.5.5 첨단 연구로

## 6.6 정리

- 단순한 RNN의 학습에서는 기울기 소실과 기울기 폭발이 문제가 된다
- 기울기 폭발에는 기울기 클리핑, 기울기 소실에는 게이트가 추가된 RNN(LSTM과 GRU등)이 효과적이다.
- LSTM에는 input게이트, forget게이트, output게이트 3개의 게이트가 있다
- 게이트에는 전용 가중치가 있으며, 시그모이드 함수를 사용하여 0.0~1.0 사이의 실수를 출력한다.
- 언어 모델 개선에는 LSTM 계층 다층화, 드롭아웃, 가중치 공유 등의 기법이 효과적이다.
- RNN의 정규화는 중요한 주제이며, 드롭아웃 기반의 다양한 기법이 제안되고 있다.



# 7. RNN을 사용한 문장 생성

## 7.1 언어 모델을 사용한 문장 생성

### 7.1.1 RNN을 사용한 문장 생성의 순서

1. - 확률이 가장 높은 단어를 선택한다. **결정적**인 방법
   - 각 후보 단어의 확률에 맞게 선택 **확률적**인 선택
     - 선택되는 단어(샘플링 단어)가 매번 다를 수 있다.
2. 두 번째 단어를 샘플링. 생성한 단어를 언어 모델에 입력하여 다음 단어의 확률분포를 얻는다.
3. 이 과정을 원하는 만큼(또는 <eos>와 같은 종결 기호까지) 반복하여 문장을 생성한다.

### 7.1.2 문장 생성 구현

### 7.1.3 더 좋은 문장으로

## 7. 2 seq2seq

시계열 데이터를 또 다른 시계열 데이터로 변환하는 문제

2개의 RNN을 이용한다.

### 7.2.1 seq2seq의 원리

![1564371304310](C:\Users\skarn\AppData\Roaming\Typora\typora-user-images\1564371304310.png)

![1564372081671](C:\Users\skarn\AppData\Roaming\Typora\typora-user-images\1564372081671.png)

Encoder 와 Decoder로는 RNN을 사용할 수 있다.

- Encoder 는 시계열 데이터를 $h$라는 은닉 상태 벡터로 변환
- 마지막 은닉 상태 $h$에 입력 문장을 번역하는 데 필요한 정보가 인코딩
  - $h$는 고정 길이 벡터다. 인코딩은 즉 임의 길이 문장을 고정 길이 벡터로 변환한다.



- Decoder는 앞 절의 신경망과 같은 구성
  - 단, **LSTM계층이 $h$를 입력받는다**
- <eos>는 `구분자`로 decoder에 문장 생성의 시작을 알린다.

- LSTM계층의 은닉 상태를 통해 순전파, 역전파를 수행한다.

### 7.2.2 시계열 데이터 변환용 장난감 문제

- 덧셈 예제들을 해보쟈. seq2seq은 덧셈을 학습한다.
- `문자`단위로 분할해보자

### 7.2.3 가변 길이 시계열 데이터

- 덧셈 문장이나 그 대답의 문자 수가 문제마다 다르다...ㅠㅠ

- **패딩(padding)**을 사용하는 방법
  - 원래 데이터에 의미 없는 데이터를 채워 모든 데이터의 길이를 균일하게 맞춘다.
  - ![1564372483151](C:\Users\skarn\AppData\Roaming\Typora\typora-user-images\1564372483151.png)
  - 출력은 앞에 구분자로 밑줄(_)을 붙이자. 
    - decoder에 문자열을 생성하라고 알리는 신호

### 7.2.4 덧셈 데이터셋

## 7.3 seq2seq 구현

# 8. 어텐션

## 8.1 어텐션의 구조

필요한 정보에만 *주목* 할 수 있게 한다.

### 8.1.1 seq2seq의 문제점

- seq2seq 인코더의 출력은 '고정 길이의 벡터'
  - 긴 문장이 입력되어도 같은 길이의 벡터에 밀어넣어야 한다.
  - 과포화 상태...

### 8.1.2 Encoder 개선

- 인코더 출력의 길이를 입력 문장의 길이에 따라 바꾸자.
- 시각별 LSTM 계층의 은닉 상태 벡터를 모두 이용한다!!!!

![1564588488291](C:\Users\skarn\AppData\Roaming\Typora\typora-user-images\1564588488291.png)

- 각 시각의 은닉 상태에는 직전에 입력된 단어에 대한 정보가 많이 포함되어 있다.
- Encoder가 출력하는 **hs**행렬은 각 단어에 해당하는 벡터들의 집합!

> 단어의 주변 정보를 담아야 한다면 bidirectional RNN이 효과적이다.

결과적으로 Encoder는 입력 문장의 길이에 비례한 정보를 인코딩한다.

### 8.1.3 Decoder 개선 (1) Weight Sum 계층

- 단순한 seq2seq은 encoder의 마지막 hidden state vector만을 decoder로 전달

- **hs** 전부를 활용할 수 있도록 decoder 개선

- '입력과 출력의 여러 단어 중 어떤 단어끼리 관련되어 있는가'

  > alignment : 단어의 대응 관계를 나타내는 정보
  >
  > attention은 alignment를 seq2seq에 자동으로 도입한다.

- 어텐션은 필요한 정보에만 주목해여 그 정보로부터 시계열 변환한다.

![1564982498725](C:\Users\skarn\AppData\Roaming\Typora\typora-user-images\1564982498725.png)

- Decoder에 입력된 단어와 대응 관계인 단어의 벡터를 **hs**에서 골라내겠다.
  - Decoder가 'I'이면 hs에서는 '나'에 대응하는 벡터를 선택한다.
  - 선택 작업은 미분할 수 없다는 문제가 발생
  - 해결책
    - 모든 것을 선택한 후
    - 각 단어의 중요도를 나타내는 '가중치'를 별도로 계산한다.
- 가중치 **a**와 각 단어벡터 **hs**로부터 가중합을 구하여 원하는 벡터 **c**를 얻는다.

![1564983014318](C:\Users\skarn\AppData\Roaming\Typora\typora-user-images\1564983014318.png)

### 8.1.4 Decoder 개선 (2) Attention Weight

- **h**가 **hs**의 각 단어 벡터와 얼마나 '비슷한가'
  - 벡터의 내적을 이용하여 계산
  - 두 벡터가 얼마나 같은 방향을 향하고 있는가
  - 벡터 유사도의 지표가 내적이다.
- **s**는 스코어이고 정규화하기 위해 softmax 함수를 적용

![1564985880026](C:\Users\skarn\AppData\Roaming\Typora\typora-user-images\1564985880026.png)

### 8.1.5 Decoder 개선(3)

Attention Weight 계층과 Weight Sum 계층을 하나로 결합

![1564986776979](C:\Users\skarn\AppData\Roaming\Typora\typora-user-images\1564986776979.png)

- Attention Weight 계층
  - Encoder가 출력하는 각 단어 벡터 **hs**에 주목하여 해당 단어의 가중치 **a** 구함
- Weight Sum 계층
  - **a**와 **hs**의 가중합을 구하고 결과를 맥락 벡터 **c**로 출력

> 결과적으로 어텐션은 Encoder의 정보 hs에서 중요한 원소에 주목하여, 
> 
> 그것을 바탕으로 맥락 벡터를 구해 위쪽 계층으로 전파한다.

![1564987262137](C:\Users\skarn\AppData\Roaming\Typora\typora-user-images\1564987262137.png)

## 8.2 어텐션을 갖춘 seq2seq 구현

### 8.2.1 Encoder 구현

### 8.2.2 Decoder 구현

### 8.2.3 seq2seq 구현

## 8.3 어텐션 평가

### 8.3.1 날짜 형식 변환 문제

### 8.3.2 어텐션을 갖춘 seq2seq의 학습

### 8.3.3 어텐션 시각화

## 8.4 어텐션에 관한 남은 이야기

### 8.4.1 양방향 RNN

- 단어의 '주변' 정보를 균형 있게 담고 싶다
  - **Bidirectional LSTM** 사용
- 지금까지 LSTM에 더해 역방향으로 처리하는 LSTM 계층을 추가
- 두 LSTM 계층의 은닉 상태를 연결시킨 벡터를 최종 은닉 상태로 처리

![1564993427881](C:\Users\skarn\AppData\Roaming\Typora\typora-user-images\1564993427881.png)

### 8.4.2 Attention 계층 사용 방법

Attention 계층의 출력이 다음 시각의 LSTM 계층에 입력되게 할 수도 있다

### 8.4.3 seq2seq 심층화와 skip 연결

더 높은 표현력을 위해

- RNN계층을 깊게 쌓는 방법
  - Encoder와 Decoder에서는 같은 층수의 LSTM 계층을 이용한다.
- Skip connection
  - 덧셈이 핵심이다. 덧셈은 역전파시 기울기를 그대로 흘려보낸다.
  - 기울기 소실(폭발) 문제를 해결한다.

> - RNN 기울기 소실 -> LSTM, GRU
> - RNN 기울기 폭발 -> 기울기 클리핑
> - RNN 깊이 방향 기울기 소실 -> skip connection

## 8.5 어텐션 응용

어텐션은 범용적이다.

### 8.5.1 구글 신경망 기계 번역(GNMT)

### 8.5.2 트랜스포머

- RNN은 병렬 처리를 할 수 없는 단점이 있다.
- RNN이 아닌 어텐션을 이용한 모델
  - 그 중 **Self - Attention** 이 핵심이다.
    - 하나의 시계열 데이터 내에서 각 원소가 다른 원소들과 어떻게 관련되나?

![1565022277000](C:\Users\skarn\AppData\Roaming\Typora\typora-user-images\1565022277000.png)

- 트랜스포머를 이용하면 계산량이 줄고 GPU를 이용한 병렬 계산이 가능하다
- 어텐션을 RNN과의 조합 뿐만 아니라
- RNN을 대체하는 모듈로도 이용할 수 있다.

### 8.5.3 뉴럴 튜링 머신(NTM)

- 신경망에 '외부 메모리'를 이용해보자.
  - rnn의 hidden state는 고정 길이라 정보가 제한적
  - 필요한 정보를 외부 기억 장치에 저장하자
- 어텐션 seq2seq에서
  - Encoder가 입력 문장을 인코딩 *정보를 메모리에 쓴다*
  - 어텐션을 통해 정보를 Decoer가 이용 *메모리부터 정보를 읽는다*
- NTM
  - RNN외부에 정보 저장용 메모리 기능을 배치하고
  - 어텐션을 이용하여 메모리로부터필요한 정보를 읽거나 쓰는 방법

> NTM은 알고리즘의 입.출력으로부터 알고리즘 자체를 학습할 수 있다.

![1565024443276](C:\Users\skarn\AppData\Roaming\Typora\typora-user-images\1565024443276.png)

- Write Head 와 Read Head에 어텐션을 사용한다.
- 콘텐츠 기반 어텐션
  - 입력으로 주어진 벡터와 비슷한 벡터를 메모리로부터 찾아낸다.
- 위치 기반 어텐션
  - 이전 시각에서 주목한 메모리의 위치를 기준으로 그 전후로 이동

## 8.6 정리

- 번역이나 음성인식 등 한 시계열 데이터를 다른 시계열 데이터로 변환하는 작업에서는 시계열 데이터 사이의 대응 관계가 존재하는 경우가 많다
- 어텐션은 두 시계열 데이터 사이의 대응 관계를 데이터로부터 학습한다
- 어텐션에서는 벡터의 내적을 사용해 벡터 사이의 유사도를 구하고, 그 유사도를 이용한 가중합 벡터가 어텐션의 출력이 된다.
- 어텐션에서 사용하는 연산은 미분 가능하기 때문에 오차역전파법으로 학습할 수 있다.
- **어텐션이 산출하는 가중치(확률)을 시각화하면 입출력의 대응 관계를 볼 수 있다.**
  - 모델 검증에 사용해 보자
- 외부 메모리를 활용한 신경망 확장 연구 예에서는 메모리를 읽고 쓰는 데 어텐션을 사용했다.
- 