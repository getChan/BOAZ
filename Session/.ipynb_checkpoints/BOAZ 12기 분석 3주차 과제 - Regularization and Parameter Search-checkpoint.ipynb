{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vetorized gradient descent (Linear Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 가중치가 여러개로 이루어진 식을 Gradient descent로 업데이트 해봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단순 선형 회귀를 위한 임의의 데이터셋을 생성합니다.\n",
    "seed = 1215\n",
    "from sklearn.datasets import make_regression\n",
    "X, y = make_regression(n_samples=2000, n_features=5, random_state=seed, n_informative = 5, noise = 3.0)\n",
    "\n",
    "X_train = X[:1600]\n",
    "X_test = X[1600:]\n",
    "y_train = y[:1600]\n",
    "y_test = y[1600:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. 다음과 같은 산점도를 그려보세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x1000 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "for i in range(5):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.scatter(X_train[:,i], y_train, label='train')\n",
    "    plt.scatter(X_test[:,i], y_test, label='test')\n",
    "    plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. h(x) = ax1+bx2+cx3+dx4+ex5라는 식을 만들어 Gradient Descent를 이용해 mse를 최소화하는 a를 찾아보세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def h(theta,X):\n",
    "    return np.dot(X,theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 10000\n",
    "learningrate = 0.01\n",
    "y_test = y_test.reshape(400,1)\n",
    "\n",
    "# 초기값(theta_start)은 자유입니다\n",
    "def Gradientdescent(X, y, theta_start):\n",
    "    theta = theta_start\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        theta -= (learningrate/len(X))*np.dot(X.T, h(theta, X)-y)\n",
    "        \n",
    "    y_pred = h(theta,X_test)\n",
    "    r_square = r2_score(y_test, y_pred)    \n",
    "    \n",
    "    return print('Optimal \"a\" is:', theta, '\\n', 'R^2 :', r_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal \"a\" is: [12.00676592  4.97269009 95.9860066  77.67241642 99.76260561] \n",
      " R^2 : 0.9996242155640246\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "Gradientdescent(X_train,y_train, np.array([0.01,0.01,0.01,0.01,0.01]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 잘 찾았는지 Linear Regression의 coefficient(theta)와 비교해봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 12.0081X1 + 4.9736X2 + 95.9852X3 + 77.6710X4 + 99.7633X5 + 0.0374\n",
      "Mean Squared Error : 10.095756191029592\n",
      "R^2 : 0.9996237342985043\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# optimal a,b based on OLS\n",
    "print(\"y = %.4fX1 + %.4fX2 + %.4fX3 + %.4fX4 + %.4fX5 + %.4f\"%\n",
    "      (float(model.coef_[0]),float(model.coef_[1]), float(model.coef_[2]), float(model.coef_[3]), \n",
    "       float(model.coef_[4]),float(model.intercept_)))\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# r2_score\n",
    "r_square = r2_score(y_test, y_pred)\n",
    "print(\"Mean Squared Error :\",mse)\n",
    "print(\"R^2 :\",r_square)\n",
    "\n",
    "# OLS 방식으로 찾아낸 coefficient값과 매우 유사합니다\n",
    "# 우리가 찾은 모델의 R^2 값이 0.0000005 정도 높습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 test set에서 왜 오차 제곱합을 최소화하는 선형식보다 gradient descent로 찾은 선형식이 설명력이 높을까요? 그 이유를 설명해주세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your Answer Here\n",
    "- 선형 관계가 아니기 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 위 경우가 항상 가능한가요?(True/False) 를 고르고 그 이유를 설명해주세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your Answer Here\n",
    "- False, Gradient Descent에서\n",
    "- Learning rate가 작아서 Local Minimum에 빠지거나 \n",
    "- Learning rate가 커서 최솟값을 찾지 못할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Linear Regression with L1,L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://i2.wp.com/laid.delanover.com/wp-content/uploads/2018/01/reg_formulas.png?w=550)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존의 Loss는 다음과 같습니다\n",
    "def MSE(mytheta,X,y):\n",
    "    return float((1./(len(X)*2)) * np.dot((h(mytheta,X)-y).T,(h(mytheta,X)-y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. L1 reg term을 추가한 MSE 함수를 작성해주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(mytheta,X,y,lambdaa):\n",
    "    return float((1./(len(X)*2)) * np.dot((h(mytheta,X)-y).T,(h(mytheta,X)-y))+lambdaa*np.sum(np.abs(mytheta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. 1-2에서 작성한 Gradientdescent 함수를 이용해 L1 reg term이 추가된 함수를 작성해주세요(람다=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 10000\n",
    "learningrate = 0.01\n",
    "lambdaa = 0.01\n",
    "y_test = y_test.reshape(400,1)\n",
    "\n",
    "# 초기값(theta_start)은 자유입니다\n",
    "def Gradientdescent(X, y, theta_start):\n",
    "    theta = theta_start\n",
    "    \n",
    "    for meaningless in range(iterations):\n",
    "        theta -= (learningrate/len(X))*np.dot(X.T, h(theta, X)-y)+lambdaa*np.abs(theta)/theta\n",
    "        \n",
    "    y_pred = h(theta,X_test)\n",
    "    r_square = r2_score(y_test, y_pred)    \n",
    "    \n",
    "    return print('Optimal \"a\" is:', theta, '\\n', 'R^2 :', r_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal \"a\" is: [11.02836336  4.0719141  94.97654022 76.82239145 98.75851635] \n",
      " R^2 : 0.9994294854560567\n"
     ]
    }
   ],
   "source": [
    "Gradientdescent(X_train,y_train, np.array([0.01,0.01,0.01,0.01,0.01]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3. 이번에는 L2 reg term이 추가된 함수로 Gradient descent를 적용해 보세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 10000\n",
    "learningrate = 0.001\n",
    "lambdaa = 0.01\n",
    "y_test = y_test.reshape(400,1)\n",
    "\n",
    "\n",
    "# 초기값(theta_start)은 자유입니다\n",
    "def Gradientdescent(X, y, theta_start):\n",
    "    theta = theta_start\n",
    "    \n",
    "    for meaningless in range(iterations):\n",
    "        error = np.zeros(5)\n",
    "                \n",
    "        for i in range(len(theta)):\n",
    "            error[i] = (1/len(X)*2)*np.sum((h(theta,X)-y)*X[:,i]) + 2*lambdaa*theta[i] \n",
    "                \n",
    "        for i in range(len(theta)):\n",
    "            theta[i] = theta[i] - learningrate*error[i]\n",
    "        \n",
    "    y_pred = h(theta,X_test)\n",
    "    r_square = r2_score(y_test, y_pred)    \n",
    "    \n",
    "    return print('Optimal \"a\" is:', theta, '\\n', 'R^2 :', r_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal \"a\" is: [11.89261968  4.93526689 95.01306251 76.99449154 98.79860667] \n",
      " R^2 : 0.9995116407526918\n"
     ]
    }
   ],
   "source": [
    "Gradientdescent(X_train,y_train, np.array([0.01,0.01,0.01,0.01,0.01]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-4. L1과 L2 regularization의 차이점을 gradient를 이용해 설명해보세요 왜 L1은 feature selection이 가능하고 L2는 그렇지 않은 걸까요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your Answer Here\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Hyperparameter Search - Grid Search vs Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "seed = 1215\n",
    "X, y = sklearn.datasets.make_classification(n_samples=3000, n_features=10, n_classes=3, n_informative = 7, random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of training set: 1687,  size of validation set: 563,  size of test set: 750\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 전체 data set을 (training + validation), test으로 나눔\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, random_state=seed)\n",
    "\n",
    "# (training + validation) set을 training, validation으로 나눔\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_trainval, y_trainval, random_state=seed)\n",
    "\n",
    "print(\"size of training set: %d,  size of validation set: %d,  size of test set: %d\" % (\n",
    "        X_train.shape[0], X_valid.shape[0], X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 SVM - grid search를 이용해 최적의 gamma와 C를 찾아봅시다 여기서 gamma와 C는 hyperparameter입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최고 점수(모델 정확도): 0.9182948490230906\n",
      "최적 매개변수: {'gamma': 0.11, 'c': 0.41000000000000003}\n"
     ]
    }
   ],
   "source": [
    "# sklearn.svm.SVC의 argument에 대한 정보는 다음의 링크에서 찾아보세요\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "\n",
    "# 기본적인 절차는 다음과 같습니다\n",
    "# 우선 아주 큰 범위에서 gamma와 C의 조합을 탐색합니다 (ex.0.001~100)\n",
    "# 특정 범위(ex.0.01~0.1)에서 best score를 달성할 경우 해당 범위에서 다시 grid search를 수행합니다.\n",
    "# 이를 반복하며 best hyperparameter를 탐색합니다\n",
    "best_score = 0\n",
    "best_parameters = {'gamma':-1, 'c':-1}\n",
    "# search할 parameter를 임의(로 설정\n",
    "for gamma, c in permutations(np.arange(10e-3, 1, 10e-2), 2):\n",
    "        \n",
    "        # 매개변수 각 조합에 대해 SVC 훈련    \n",
    "        svm = SVC(gamma=gamma, C=c)\n",
    "        \n",
    "        # train set으로 model fitting\n",
    "        svm.fit(X_train, y_train)\n",
    "        \n",
    "        # validation set으로 SVC 평가         \n",
    "        score = svm.score(X_valid, y_valid) \n",
    "        \n",
    "        # 점수가 더 높으면 기록 \n",
    "        if score > best_score :\n",
    "            best_score = score\n",
    "            best_parameters['gamma'], best_parameters['c'] = gamma, c\n",
    "        \n",
    "print(\"최고 점수(모델 정확도):\", best_score)\n",
    "print(\"최적 매개변수:\", best_parameters)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 SVM - ransom search를 이용해 최적의 gamma,C를 찾아봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최고 점수(모델 정확도): 0.9218472468916519\n",
      "최적 매개변수: {'gamma': 0.049680875577510636, 'c': 3.4429011754295726}\n"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "best_parameters = {'gamma':-1, 'c':-1}\n",
    "# search할 parameter를 임의(random)로 설정\n",
    "for _ in range(100):\n",
    "        gamma = 10**random.uniform(-3, 3)\n",
    "        c = 10**random.uniform(-3, 3)\n",
    "        # 매개변수 각 조합에 대해 SVC 훈련    \n",
    "        svm = SVC(gamma=gamma, C=c)\n",
    "        \n",
    "        # train set으로 model fitting\n",
    "        svm.fit(X_train, y_train)\n",
    "        \n",
    "        # validation set으로 SVC 평가         \n",
    "        score = svm.score(X_valid, y_valid) \n",
    "        \n",
    "        # 점수가 더 높으면 기록 \n",
    "        if score > best_score :\n",
    "            best_score = score\n",
    "            best_parameters['gamma'], best_parameters['c'] = gamma, c\n",
    "            \n",
    "print(\"최고 점수(모델 정확도):\", best_score)\n",
    "print(\"최적 매개변수:\", best_parameters)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 100번의 탐색 기회를 이용해 Grid Search와 Random Search를 각각 수행해보고 최적의 hyperparameter 조합을 찾아보세요 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최고 점수(모델 정확도): 0.9200710479573713\n",
      "최적 매개변수: {'gamma': 0.08441936451774122, 'c': 0.6670424303008043}\n"
     ]
    }
   ],
   "source": [
    "# random search\n",
    "best_score = 0\n",
    "best_parameters = {'gamma':-1, 'c':-1}\n",
    "# search할 parameter를 임의(random)로 설정\n",
    "for _ in range(100):\n",
    "        gamma = 10**random.uniform(-2, 0)\n",
    "        c = 10**random.uniform(-2, 0)\n",
    "        # 매개변수 각 조합에 대해 SVC 훈련    \n",
    "        svm = SVC(gamma=gamma, C=c)\n",
    "        \n",
    "        # train set으로 model fitting\n",
    "        svm.fit(X_train, y_train)\n",
    "        \n",
    "        # validation set으로 SVC 평가         \n",
    "        score = svm.score(X_valid, y_valid) \n",
    "        \n",
    "        # 점수가 더 높으면 기록 \n",
    "        if score > best_score :\n",
    "            best_score = score\n",
    "            best_parameters['gamma'], best_parameters['c'] = gamma, c\n",
    "            \n",
    "print(\"최고 점수(모델 정확도):\", best_score)\n",
    "print(\"최적 매개변수:\", best_parameters)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최고 점수(모델 정확도): 0.9200710479573713\n",
      "최적 매개변수: {'gamma': 0.1, 'c': 0.8}\n"
     ]
    }
   ],
   "source": [
    "# grid search\n",
    "best_score = 0\n",
    "best_parameters = {'gamma':-1, 'c':-1}\n",
    "# search할 parameter를 임의(로 설정\n",
    "for gamma, c in permutations(np.arange(10e-2, 10e-1, 10e-2), 2):\n",
    "        \n",
    "        # 매개변수 각 조합에 대해 SVC 훈련    \n",
    "        svm = SVC(gamma=gamma, C=c)\n",
    "        \n",
    "        # train set으로 model fitting\n",
    "        svm.fit(X_train, y_train)\n",
    "        \n",
    "        # validation set으로 SVC 평가         \n",
    "        score = svm.score(X_valid, y_valid) \n",
    "        \n",
    "        # 점수가 더 높으면 기록 \n",
    "        if score > best_score :\n",
    "            best_score = score\n",
    "            best_parameters['gamma'], best_parameters['c'] = gamma, c\n",
    "        \n",
    "print(\"최고 점수(모델 정확도):\", best_score)\n",
    "print(\"최적 매개변수:\", best_parameters)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 다음의 그림을 참고하여 어떤 parameter search 방법이 더 좋은 것인지 설명해주세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://i.stack.imgur.com/cIDuR.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your Answer Here\n",
    "- Grid Search가 더 좋아보입니다.\n",
    "- Grid Search는 step이 크면 최적 parameter를 지나치게 될 수 있는데 비해\n",
    "- Random Search는 step에 관계없이 random한 parameter를 지정하므로 신뢰성이 떨어집니다.\n",
    "- Grid Search의 step을 줄여가며 parameter를 찾는 것이 좋아보입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 고생하셨습니다 :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
